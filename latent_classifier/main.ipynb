{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload dataset kaggle dataset to huggingface "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the specific Hugging Face dataset with the name `stanpony/european_credit_card_dataset` doesn't exist, we upload the CSV from the Kaggle dataset to Hugging Face. Otherwise, we create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset stanpony/european_credit_card_fraud_dataset already exists.\n",
      "Loaded train and validation splits.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load the Hugging Face API token from the .env file\n",
    "load_dotenv()\n",
    "hf_api_token = os.getenv('HF_API_TOKEN')\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=hf_api_token)\n",
    "\n",
    "dataset_name = \"stanpony/european_credit_card_fraud_dataset\"\n",
    "try:\n",
    "    # Check if the dataset exists by trying to load the train split\n",
    "    train_dataset = load_dataset(dataset_name, split='train')\n",
    "    vali_dataset = load_dataset(dataset_name, split='validation')\n",
    "    print(f\"Dataset {dataset_name} already exists.\")\n",
    "    print(\"Loaded train and validation splits.\")\n",
    "except:\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(\"data/creditcard.csv\")\n",
    "\n",
    "    # Remove duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Select all rows where Class == 1 (fraud cases)\n",
    "    df_class_1 = df[df['Class'] == 1].copy()\n",
    "    df_class_1['original_index'] = df_class_1.index\n",
    "\n",
    "    # Select 15,000 random rows where Class == 0 (non-fraud cases)\n",
    "    df_class_0 = df[df['Class'] == 0].sample(n=15000, random_state=42).copy()\n",
    "    df_class_0['original_index'] = df_class_0.index\n",
    "\n",
    "    # Reorder columns to make 'original_index' the first column\n",
    "    df_class_1 = df_class_1[['original_index'] + [col for col in df_class_1.columns if col != 'original_index']]\n",
    "    df_class_0 = df_class_0[['original_index'] + [col for col in df_class_0.columns if col != 'original_index']]\n",
    "\n",
    "    # Combine the two dataframes\n",
    "    new_df = pd.concat([df_class_1, df_class_0]).reset_index(drop=True)\n",
    "\n",
    "    # Split the dataframe into train and a temporary set (33% of the data), stratifying by 'Class'\n",
    "    train_df, temp_df = train_test_split(new_df, test_size=0.2, stratify=new_df['Class'], random_state=42)\n",
    "\n",
    "    # Further split the temporary set into test and validation sets:\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['Class'], random_state=42)\n",
    "\n",
    "    # Save the train, test, and validation sets to CSV files\n",
    "    train_df.to_csv('data/train_creditcard.csv', index=False)\n",
    "    test_df.to_csv('data/test_creditcard.csv', index=False)\n",
    "    val_df.to_csv('data/val_creditcard.csv', index=False)\n",
    "\n",
    "    # Create a dictionary mapping split names to file paths\n",
    "    data_files = {\n",
    "        \"train\": \"data/train_creditcard.csv\",\n",
    "        \"test\": \"data/test_creditcard.csv\",\n",
    "        \"validation\": \"data/val_creditcard.csv\"\n",
    "    }\n",
    "\n",
    "    # Create a DatasetDict with train, test, and validation splits\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_csv(data_files[\"train\"]),\n",
    "        \"test\": Dataset.from_csv(data_files[\"test\"]),\n",
    "        \"validation\": Dataset.from_csv(data_files[\"validation\"])\n",
    "    })\n",
    "\n",
    "    # Push the dataset to Hugging Face Hub\n",
    "    dataset.push_to_hub(dataset_name)\n",
    "    print(f\"Dataset {dataset_name} created and uploaded.\")\n",
    "\n",
    "    # Load the train dataset from the Hub for verification\n",
    "    train_dataset = load_dataset(dataset_name, split='train')\n",
    "    print(f\"Dataset train loaded.\")\n",
    "    vali_dataset = load_dataset(dataset_name, split='validation')\n",
    "    print(f\"Dataset validation loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE (latent feature extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Fraud VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkstern/miniconda3/envs/credit_vae/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from configs/fraud_vae/vae_test.config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 16:17:10,144 [INFO] Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset to class 1: 378 samples\n",
      "Normalization statistics (calculated from class 1): {'Time': {'mean': 80790.48148148147, 'std': 48332.5139872635}, 'Amount': {'mean': 133.6764814814815, 'std': 276.3532237447719}}\n",
      "Filtered dataset to class 1: 378 samples\n",
      "Filtered dataset to class 1: 47 samples\n",
      "Filtered dataset to class 1: 48 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alexkstern/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexkstern\u001b[0m (\u001b[33malexksternteam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexkstern/fraudml/wandb/run-20250305_161714-6ho7ce2x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexkstern/fraud-vae/runs/6ho7ce2x' target=\"_blank\">vae-fraud-training</a></strong> to <a href='https://wandb.ai/alexkstern/fraud-vae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexkstern/fraud-vae' target=\"_blank\">https://wandb.ai/alexkstern/fraud-vae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexkstern/fraud-vae/runs/6ho7ce2x' target=\"_blank\">https://wandb.ai/alexkstern/fraud-vae/runs/6ho7ce2x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 16:17:17,804 [INFO] WandB initialized: project=fraud-vae, run=vae-fraud-training\n",
      "2025-03-05 16:17:17,809 [INFO] Starting training\n",
      "Epoch 1/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 63.90it/s, loss=427, recon=414, kl=12.8]       \n",
      "Epoch 1/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 28.77it/s, val_loss=959]\n",
      "2025-03-05 16:17:18,791 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch1_val419.7570.pt\n",
      "2025-03-05 16:17:19,039 [INFO] Epoch 1/50 - Time: 1.23s\n",
      "2025-03-05 16:17:19,042 [INFO]   Train Loss: 458.753124 (Recon: 453.634746, KL: 5.118378)\n",
      "2025-03-05 16:17:19,044 [INFO]   Val Loss: 419.757005 (Recon: 404.048797, KL: 15.708206)\n",
      "Epoch 2/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 65.29it/s, loss=232, recon=224, kl=8.23]  \n",
      "Epoch 2/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 27.60it/s, val_loss=727]\n",
      "2025-03-05 16:17:20,024 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch2_val292.4472.pt\n",
      "2025-03-05 16:17:20,224 [INFO] Epoch 2/50 - Time: 1.18s\n",
      "2025-03-05 16:17:20,228 [INFO]   Train Loss: 225.687722 (Recon: 216.650211, KL: 9.037511)\n",
      "2025-03-05 16:17:20,231 [INFO]   Val Loss: 292.447151 (Recon: 281.439484, KL: 11.007658)\n",
      "Epoch 3/50 [Train]: 100%|██████████| 48/48 [00:02<00:00, 21.72it/s, loss=231, recon=220, kl=10.6]  \n",
      "Epoch 3/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 26.87it/s, val_loss=616]\n",
      "2025-03-05 16:17:22,696 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch3_val226.2278.pt\n",
      "2025-03-05 16:17:22,902 [INFO] Epoch 3/50 - Time: 2.67s\n",
      "2025-03-05 16:17:22,904 [INFO]   Train Loss: 171.938868 (Recon: 162.780902, KL: 9.157967)\n",
      "2025-03-05 16:17:22,907 [INFO]   Val Loss: 226.227839 (Recon: 214.376188, KL: 11.851654)\n",
      "Epoch 4/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 59.09it/s, loss=300, recon=285, kl=14.8]  \n",
      "Epoch 4/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 30.24it/s, val_loss=529] \n",
      "2025-03-05 16:17:23,943 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch4_val182.8845.pt\n",
      "2025-03-05 16:17:24,130 [INFO] Epoch 4/50 - Time: 1.22s\n",
      "2025-03-05 16:17:24,132 [INFO]   Train Loss: 166.568470 (Recon: 156.517147, KL: 10.051322)\n",
      "2025-03-05 16:17:24,134 [INFO]   Val Loss: 182.884502 (Recon: 169.175560, KL: 13.708943)\n",
      "Epoch 5/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 59.16it/s, loss=145, recon=131, kl=13.8]  \n",
      "Epoch 5/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.93it/s, val_loss=519] \n",
      "2025-03-05 16:17:25,158 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch5_val175.6444.pt\n",
      "2025-03-05 16:17:25,358 [INFO] Epoch 5/50 - Time: 1.22s\n",
      "2025-03-05 16:17:25,359 [INFO]   Train Loss: 140.062501 (Recon: 128.693724, KL: 11.368776)\n",
      "2025-03-05 16:17:25,361 [INFO]   Val Loss: 175.644397 (Recon: 160.030234, KL: 15.614162)\n",
      "Epoch 6/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 59.38it/s, loss=471, recon=461, kl=10.6]  \n",
      "Epoch 6/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 32.72it/s, val_loss=591] \n",
      "2025-03-05 16:17:26,379 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch6_val167.7335.pt\n",
      "2025-03-05 16:17:26,561 [INFO] Epoch 6/50 - Time: 1.20s\n",
      "2025-03-05 16:17:26,563 [INFO]   Train Loss: 141.573047 (Recon: 129.193119, KL: 12.379929)\n",
      "2025-03-05 16:17:26,566 [INFO]   Val Loss: 167.733537 (Recon: 148.513552, KL: 19.219984)\n",
      "Epoch 7/50 [Train]: 100%|██████████| 48/48 [00:01<00:00, 25.04it/s, loss=163, recon=149, kl=14.2]  \n",
      "Epoch 7/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 25.40it/s, val_loss=365] \n",
      "2025-03-05 16:17:28,746 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch7_val134.7454.pt\n",
      "2025-03-05 16:17:28,934 [INFO] Epoch 7/50 - Time: 2.37s\n",
      "2025-03-05 16:17:28,936 [INFO]   Train Loss: 132.024782 (Recon: 119.789180, KL: 12.235603)\n",
      "2025-03-05 16:17:28,938 [INFO]   Val Loss: 134.745385 (Recon: 119.961917, KL: 14.783471)\n",
      "Epoch 8/50 [Train]: 100%|██████████| 48/48 [00:01<00:00, 42.08it/s, loss=101, recon=84.6, kl=16.3] \n",
      "Epoch 8/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 19.24it/s, val_loss=342] \n",
      "2025-03-05 16:17:30,418 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch8_val128.9658.pt\n",
      "2025-03-05 16:17:30,618 [INFO] Epoch 8/50 - Time: 1.68s\n",
      "2025-03-05 16:17:30,623 [INFO]   Train Loss: 119.269930 (Recon: 106.726757, KL: 12.543174)\n",
      "2025-03-05 16:17:30,625 [INFO]   Val Loss: 128.965754 (Recon: 112.365446, KL: 16.600306)\n",
      "Epoch 9/50 [Train]: 100%|██████████| 48/48 [00:01<00:00, 38.44it/s, loss=129, recon=113, kl=16.3]  \n",
      "Epoch 9/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 10.19it/s, val_loss=256] \n",
      "2025-03-05 16:17:32,499 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch9_val118.9159.pt\n",
      "2025-03-05 16:17:32,743 [INFO] Epoch 9/50 - Time: 2.11s\n",
      "2025-03-05 16:17:32,747 [INFO]   Train Loss: 117.115162 (Recon: 103.528667, KL: 13.586495)\n",
      "2025-03-05 16:17:32,748 [INFO]   Val Loss: 118.915914 (Recon: 104.737839, KL: 14.178076)\n",
      "Epoch 10/50 [Train]: 100%|██████████| 48/48 [00:01<00:00, 45.33it/s, loss=206, recon=191, kl=15.4]  \n",
      "Epoch 10/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 33.22it/s, val_loss=252] \n",
      "2025-03-05 16:17:34,016 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch10_val105.4147.pt\n",
      "2025-03-05 16:17:34,258 [INFO] Epoch 10/50 - Time: 1.50s\n",
      "2025-03-05 16:17:34,260 [INFO]   Train Loss: 125.966554 (Recon: 112.366976, KL: 13.599580)\n",
      "2025-03-05 16:17:34,262 [INFO]   Val Loss: 105.414695 (Recon: 88.747001, KL: 16.667695)\n",
      "Epoch 11/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 65.40it/s, loss=133, recon=118, kl=15.6]  \n",
      "Epoch 11/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.91it/s, val_loss=263] \n",
      "2025-03-05 16:17:35,202 [INFO] Epoch 11/50 - Time: 0.94s\n",
      "2025-03-05 16:17:35,203 [INFO]   Train Loss: 126.115085 (Recon: 111.523805, KL: 14.591280)\n",
      "2025-03-05 16:17:35,205 [INFO]   Val Loss: 106.337730 (Recon: 87.606290, KL: 18.731441)\n",
      "Epoch 12/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 64.78it/s, loss=293, recon=278, kl=15.7]  \n",
      "Epoch 12/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 36.94it/s, val_loss=183] \n",
      "2025-03-05 16:17:36,130 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch12_val88.8960.pt\n",
      "2025-03-05 16:17:36,446 [INFO] Epoch 12/50 - Time: 1.24s\n",
      "2025-03-05 16:17:36,449 [INFO]   Train Loss: 105.747858 (Recon: 91.241845, KL: 14.506012)\n",
      "2025-03-05 16:17:36,451 [INFO]   Val Loss: 88.896047 (Recon: 71.555980, KL: 17.340067)\n",
      "Epoch 13/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 60.84it/s, loss=404, recon=390, kl=14.2]  \n",
      "Epoch 13/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 30.01it/s, val_loss=278] \n",
      "2025-03-05 16:17:37,456 [INFO] Epoch 13/50 - Time: 1.00s\n",
      "2025-03-05 16:17:37,458 [INFO]   Train Loss: 114.222682 (Recon: 99.458622, KL: 14.764060)\n",
      "2025-03-05 16:17:37,460 [INFO]   Val Loss: 98.682143 (Recon: 80.323819, KL: 18.358325)\n",
      "Epoch 14/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 65.87it/s, loss=1.13e+3, recon=1.11e+3, kl=18.4]\n",
      "Epoch 14/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 29.08it/s, val_loss=163] \n",
      "2025-03-05 16:17:38,412 [INFO] Epoch 14/50 - Time: 0.95s\n",
      "2025-03-05 16:17:38,414 [INFO]   Train Loss: 118.462894 (Recon: 104.117369, KL: 14.345525)\n",
      "2025-03-05 16:17:38,415 [INFO]   Val Loss: 108.563930 (Recon: 88.171210, KL: 20.392719)\n",
      "Epoch 15/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 65.96it/s, loss=488, recon=471, kl=17.5]  \n",
      "Epoch 15/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 30.76it/s, val_loss=265] \n",
      "2025-03-05 16:17:39,354 [INFO] Epoch 15/50 - Time: 0.94s\n",
      "2025-03-05 16:17:39,356 [INFO]   Train Loss: 109.876198 (Recon: 95.069187, KL: 14.807010)\n",
      "2025-03-05 16:17:39,357 [INFO]   Val Loss: 103.524136 (Recon: 84.480714, KL: 19.043422)\n",
      "Epoch 16/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 64.67it/s, loss=362, recon=347, kl=15]    \n",
      "Epoch 16/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 32.27it/s, val_loss=184] \n",
      "2025-03-05 16:17:40,306 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch16_val85.8678.pt\n",
      "2025-03-05 16:17:40,491 [INFO] Epoch 16/50 - Time: 1.13s\n",
      "2025-03-05 16:17:40,493 [INFO]   Train Loss: 99.710688 (Recon: 84.729951, KL: 14.980737)\n",
      "2025-03-05 16:17:40,495 [INFO]   Val Loss: 85.867814 (Recon: 66.193429, KL: 19.674385)\n",
      "Epoch 17/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 67.18it/s, loss=311, recon=297, kl=13.8]  \n",
      "Epoch 17/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 29.65it/s, val_loss=247] \n",
      "2025-03-05 16:17:41,428 [INFO] Epoch 17/50 - Time: 0.93s\n",
      "2025-03-05 16:17:41,430 [INFO]   Train Loss: 103.747476 (Recon: 88.692619, KL: 15.054858)\n",
      "2025-03-05 16:17:41,432 [INFO]   Val Loss: 93.312138 (Recon: 72.896192, KL: 20.415947)\n",
      "Epoch 18/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 63.96it/s, loss=210, recon=194, kl=16]    \n",
      "Epoch 18/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.83it/s, val_loss=146] \n",
      "2025-03-05 16:17:42,394 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch18_val81.4968.pt\n",
      "2025-03-05 16:17:42,572 [INFO] Epoch 18/50 - Time: 1.14s\n",
      "2025-03-05 16:17:42,574 [INFO]   Train Loss: 90.629409 (Recon: 75.782319, KL: 14.847090)\n",
      "2025-03-05 16:17:42,577 [INFO]   Val Loss: 81.496807 (Recon: 65.166371, KL: 16.330435)\n",
      "Epoch 19/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 59.29it/s, loss=2.1e+3, recon=2.08e+3, kl=21.6]\n",
      "Epoch 19/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.06it/s, val_loss=147] \n",
      "2025-03-05 16:17:43,598 [INFO] Epoch 19/50 - Time: 1.02s\n",
      "2025-03-05 16:17:43,600 [INFO]   Train Loss: 104.225035 (Recon: 88.798721, KL: 15.426313)\n",
      "2025-03-05 16:17:43,601 [INFO]   Val Loss: 94.177613 (Recon: 80.212210, KL: 13.965402)\n",
      "Epoch 20/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 68.38it/s, loss=313, recon=297, kl=16.5]  \n",
      "Epoch 20/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 32.83it/s, val_loss=122] \n",
      "2025-03-05 16:17:44,507 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch20_val76.9334.pt\n",
      "2025-03-05 16:17:44,709 [INFO] Epoch 20/50 - Time: 1.11s\n",
      "2025-03-05 16:17:44,711 [INFO]   Train Loss: 90.364223 (Recon: 75.154067, KL: 15.210156)\n",
      "2025-03-05 16:17:44,714 [INFO]   Val Loss: 76.933404 (Recon: 59.098132, KL: 17.835272)\n",
      "Epoch 21/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 69.09it/s, loss=123, recon=102, kl=20.4]  \n",
      "Epoch 21/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.87it/s, val_loss=172] \n",
      "2025-03-05 16:17:45,613 [INFO] Epoch 21/50 - Time: 0.90s\n",
      "2025-03-05 16:17:45,615 [INFO]   Train Loss: 100.156660 (Recon: 84.401763, KL: 15.754898)\n",
      "2025-03-05 16:17:45,617 [INFO]   Val Loss: 82.025679 (Recon: 60.622486, KL: 21.403194)\n",
      "Epoch 22/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 69.13it/s, loss=134, recon=117, kl=17.4]  \n",
      "Epoch 22/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 33.59it/s, val_loss=99.8]\n",
      "2025-03-05 16:17:46,508 [INFO] Epoch 22/50 - Time: 0.89s\n",
      "2025-03-05 16:17:46,509 [INFO]   Train Loss: 113.001286 (Recon: 97.367845, KL: 15.633440)\n",
      "2025-03-05 16:17:46,510 [INFO]   Val Loss: 79.735196 (Recon: 62.775781, KL: 16.959417)\n",
      "Epoch 23/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 70.46it/s, loss=123, recon=107, kl=16.1]  \n",
      "Epoch 23/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 27.97it/s, val_loss=118] \n",
      "2025-03-05 16:17:47,432 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch23_val76.2138.pt\n",
      "2025-03-05 16:17:47,631 [INFO] Epoch 23/50 - Time: 1.12s\n",
      "2025-03-05 16:17:47,633 [INFO]   Train Loss: 96.856025 (Recon: 81.392778, KL: 15.463247)\n",
      "2025-03-05 16:17:47,635 [INFO]   Val Loss: 76.213760 (Recon: 59.542527, KL: 16.671235)\n",
      "Epoch 24/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 64.07it/s, loss=327, recon=307, kl=19.6]  \n",
      "Epoch 24/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 32.74it/s, val_loss=98.1]\n",
      "2025-03-05 16:17:48,585 [INFO] Epoch 24/50 - Time: 0.95s\n",
      "2025-03-05 16:17:48,586 [INFO]   Train Loss: 88.534221 (Recon: 72.790262, KL: 15.743959)\n",
      "2025-03-05 16:17:48,587 [INFO]   Val Loss: 83.869669 (Recon: 68.173196, KL: 15.696475)\n",
      "Epoch 25/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 62.30it/s, loss=232, recon=217, kl=14.7]  \n",
      "Epoch 25/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 32.18it/s, val_loss=90.1]\n",
      "2025-03-05 16:17:49,567 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch25_val75.7362.pt\n",
      "2025-03-05 16:17:49,749 [INFO] Epoch 25/50 - Time: 1.16s\n",
      "2025-03-05 16:17:49,752 [INFO]   Train Loss: 100.205041 (Recon: 84.992234, KL: 15.212807)\n",
      "2025-03-05 16:17:49,754 [INFO]   Val Loss: 75.736162 (Recon: 57.669786, KL: 18.066376)\n",
      "Epoch 26/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 67.16it/s, loss=362, recon=344, kl=17.9]  \n",
      "Epoch 26/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.44it/s, val_loss=104] \n",
      "2025-03-05 16:17:50,683 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch26_val71.3467.pt\n",
      "2025-03-05 16:17:50,868 [INFO] Epoch 26/50 - Time: 1.11s\n",
      "2025-03-05 16:17:50,871 [INFO]   Train Loss: 87.328762 (Recon: 72.261496, KL: 15.067265)\n",
      "2025-03-05 16:17:50,872 [INFO]   Val Loss: 71.346749 (Recon: 54.342257, KL: 17.004491)\n",
      "Epoch 27/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 65.42it/s, loss=462, recon=440, kl=22.7]  \n",
      "Epoch 27/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 33.12it/s, val_loss=85.3]\n",
      "2025-03-05 16:17:51,814 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch27_val70.8439.pt\n",
      "2025-03-05 16:17:52,066 [INFO] Epoch 27/50 - Time: 1.19s\n",
      "2025-03-05 16:17:52,069 [INFO]   Train Loss: 99.556539 (Recon: 83.470775, KL: 16.085764)\n",
      "2025-03-05 16:17:52,071 [INFO]   Val Loss: 70.843886 (Recon: 53.416075, KL: 17.427811)\n",
      "Epoch 28/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 68.72it/s, loss=238, recon=222, kl=15.7]  \n",
      "Epoch 28/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 33.15it/s, val_loss=82.9]\n",
      "2025-03-05 16:17:52,974 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch28_val63.8823.pt\n",
      "2025-03-05 16:17:53,163 [INFO] Epoch 28/50 - Time: 1.09s\n",
      "2025-03-05 16:17:53,165 [INFO]   Train Loss: 98.740881 (Recon: 82.336070, KL: 16.404811)\n",
      "2025-03-05 16:17:53,166 [INFO]   Val Loss: 63.882290 (Recon: 43.406674, KL: 20.475616)\n",
      "Epoch 29/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 69.80it/s, loss=111, recon=92.9, kl=18]   \n",
      "Epoch 29/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.82it/s, val_loss=85.7]\n",
      "2025-03-05 16:17:54,059 [INFO] Epoch 29/50 - Time: 0.89s\n",
      "2025-03-05 16:17:54,060 [INFO]   Train Loss: 97.362825 (Recon: 81.648297, KL: 15.714528)\n",
      "2025-03-05 16:17:54,062 [INFO]   Val Loss: 64.744826 (Recon: 45.797257, KL: 18.947568)\n",
      "Epoch 30/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 62.92it/s, loss=126, recon=108, kl=17.4]  \n",
      "Epoch 30/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 33.57it/s, val_loss=101] \n",
      "2025-03-05 16:17:55,020 [INFO] Epoch 30/50 - Time: 0.96s\n",
      "2025-03-05 16:17:55,021 [INFO]   Train Loss: 105.684582 (Recon: 90.020194, KL: 15.664389)\n",
      "2025-03-05 16:17:55,023 [INFO]   Val Loss: 84.097150 (Recon: 67.097165, KL: 16.999985)\n",
      "Epoch 31/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 74.61it/s, loss=434, recon=415, kl=18.7]  \n",
      "Epoch 31/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.89it/s, val_loss=171] \n",
      "2025-03-05 16:17:55,872 [INFO] Epoch 31/50 - Time: 0.85s\n",
      "2025-03-05 16:17:55,873 [INFO]   Train Loss: 89.405504 (Recon: 73.014203, KL: 16.391301)\n",
      "2025-03-05 16:17:55,874 [INFO]   Val Loss: 83.202450 (Recon: 61.542069, KL: 21.660380)\n",
      "Epoch 32/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 68.50it/s, loss=63, recon=46.7, kl=16.2]  \n",
      "Epoch 32/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 35.28it/s, val_loss=108] \n",
      "2025-03-05 16:17:56,760 [INFO] Epoch 32/50 - Time: 0.88s\n",
      "2025-03-05 16:17:56,762 [INFO]   Train Loss: 89.379498 (Recon: 73.228825, KL: 16.150673)\n",
      "2025-03-05 16:17:56,764 [INFO]   Val Loss: 77.694014 (Recon: 58.629356, KL: 19.064657)\n",
      "Epoch 33/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 62.77it/s, loss=126, recon=109, kl=16.6]  \n",
      "Epoch 33/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 32.55it/s, val_loss=98.8]\n",
      "2025-03-05 16:17:57,729 [INFO] Epoch 33/50 - Time: 0.96s\n",
      "2025-03-05 16:17:57,730 [INFO]   Train Loss: 96.221267 (Recon: 80.215466, KL: 16.005801)\n",
      "2025-03-05 16:17:57,732 [INFO]   Val Loss: 69.296502 (Recon: 49.476864, KL: 19.819638)\n",
      "Epoch 34/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 60.50it/s, loss=250, recon=235, kl=15.5]  \n",
      "Epoch 34/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.03it/s, val_loss=134] \n",
      "2025-03-05 16:17:58,735 [INFO] Epoch 34/50 - Time: 1.00s\n",
      "2025-03-05 16:17:58,738 [INFO]   Train Loss: 90.844729 (Recon: 75.034891, KL: 15.809838)\n",
      "2025-03-05 16:17:58,740 [INFO]   Val Loss: 69.023837 (Recon: 48.943855, KL: 20.079982)\n",
      "Epoch 35/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 66.54it/s, loss=152, recon=135, kl=17.5]  \n",
      "Epoch 35/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 30.89it/s, val_loss=104] \n",
      "2025-03-05 16:17:59,672 [INFO] Epoch 35/50 - Time: 0.93s\n",
      "2025-03-05 16:17:59,675 [INFO]   Train Loss: 100.221068 (Recon: 83.781180, KL: 16.439888)\n",
      "2025-03-05 16:17:59,677 [INFO]   Val Loss: 75.911516 (Recon: 56.492958, KL: 19.418558)\n",
      "Epoch 36/50 [Train]: 100%|██████████| 48/48 [00:01<00:00, 39.15it/s, loss=446, recon=428, kl=18.2]  \n",
      "Epoch 36/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 30.43it/s, val_loss=88.7]\n",
      "2025-03-05 16:18:01,123 [INFO] Saved checkpoint to saved_models/fraud/v1/VariationalAutoEncoder_fraud_20250305-161711_epoch36_val61.1807.pt\n",
      "2025-03-05 16:18:01,337 [INFO] Epoch 36/50 - Time: 1.66s\n",
      "2025-03-05 16:18:01,339 [INFO]   Train Loss: 96.036043 (Recon: 79.864061, KL: 16.171982)\n",
      "2025-03-05 16:18:01,341 [INFO]   Val Loss: 61.180723 (Recon: 41.758145, KL: 19.422578)\n",
      "Epoch 37/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 57.62it/s, loss=349, recon=331, kl=17.6]  \n",
      "Epoch 37/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 23.46it/s, val_loss=72.6]\n",
      "2025-03-05 16:18:02,449 [INFO] Epoch 37/50 - Time: 1.11s\n",
      "2025-03-05 16:18:02,452 [INFO]   Train Loss: 111.699519 (Recon: 95.242965, KL: 16.456554)\n",
      "2025-03-05 16:18:02,454 [INFO]   Val Loss: 69.489638 (Recon: 49.793363, KL: 19.696274)\n",
      "Epoch 38/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 58.37it/s, loss=162, recon=144, kl=18.4]  \n",
      "Epoch 38/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 21.32it/s, val_loss=94.2]\n",
      "2025-03-05 16:18:03,579 [INFO] Epoch 38/50 - Time: 1.12s\n",
      "2025-03-05 16:18:03,580 [INFO]   Train Loss: 85.134664 (Recon: 69.006077, KL: 16.128587)\n",
      "2025-03-05 16:18:03,584 [INFO]   Val Loss: 87.837915 (Recon: 71.361962, KL: 16.475954)\n",
      "Epoch 39/50 [Train]: 100%|██████████| 48/48 [00:01<00:00, 40.25it/s, loss=405, recon=388, kl=17.4]  \n",
      "Epoch 39/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 27.68it/s, val_loss=68.4]\n",
      "2025-03-05 16:18:05,020 [INFO] Epoch 39/50 - Time: 1.43s\n",
      "2025-03-05 16:18:05,022 [INFO]   Train Loss: 90.742811 (Recon: 74.540851, KL: 16.201959)\n",
      "2025-03-05 16:18:05,024 [INFO]   Val Loss: 72.452982 (Recon: 52.738049, KL: 19.714934)\n",
      "Epoch 40/50 [Train]: 100%|██████████| 48/48 [00:01<00:00, 44.75it/s, loss=89.5, recon=72.8, kl=16.7]\n",
      "Epoch 40/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 22.98it/s, val_loss=125] \n",
      "2025-03-05 16:18:06,376 [INFO] Epoch 40/50 - Time: 1.35s\n",
      "2025-03-05 16:18:06,379 [INFO]   Train Loss: 105.322196 (Recon: 89.000861, KL: 16.321334)\n",
      "2025-03-05 16:18:06,381 [INFO]   Val Loss: 75.698221 (Recon: 57.684130, KL: 18.014091)\n",
      "Epoch 41/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 50.91it/s, loss=132, recon=115, kl=17.4]  \n",
      "Epoch 41/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 31.47it/s, val_loss=94.8]\n",
      "2025-03-05 16:18:07,533 [INFO] Epoch 41/50 - Time: 1.15s\n",
      "2025-03-05 16:18:07,535 [INFO]   Train Loss: 96.256679 (Recon: 80.174109, KL: 16.082571)\n",
      "2025-03-05 16:18:07,537 [INFO]   Val Loss: 71.310672 (Recon: 52.542473, KL: 18.768200)\n",
      "Epoch 42/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 58.25it/s, loss=387, recon=369, kl=17.6]  \n",
      "Epoch 42/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 33.97it/s, val_loss=73.9]\n",
      "2025-03-05 16:18:08,555 [INFO] Epoch 42/50 - Time: 1.02s\n",
      "2025-03-05 16:18:08,557 [INFO]   Train Loss: 86.234176 (Recon: 70.089284, KL: 16.144892)\n",
      "2025-03-05 16:18:08,558 [INFO]   Val Loss: 77.516933 (Recon: 60.307610, KL: 17.209322)\n",
      "Epoch 43/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 70.82it/s, loss=180, recon=161, kl=19.3]  \n",
      "Epoch 43/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 30.41it/s, val_loss=103] \n",
      "2025-03-05 16:18:09,450 [INFO] Epoch 43/50 - Time: 0.89s\n",
      "2025-03-05 16:18:09,452 [INFO]   Train Loss: 98.636079 (Recon: 82.192563, KL: 16.443515)\n",
      "2025-03-05 16:18:09,453 [INFO]   Val Loss: 75.884334 (Recon: 56.701845, KL: 19.182490)\n",
      "Epoch 44/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 63.70it/s, loss=1.98e+3, recon=1.96e+3, kl=22.6]\n",
      "Epoch 44/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 30.98it/s, val_loss=136] \n",
      "2025-03-05 16:18:10,416 [INFO] Epoch 44/50 - Time: 0.96s\n",
      "2025-03-05 16:18:10,418 [INFO]   Train Loss: 104.588501 (Recon: 88.115966, KL: 16.472535)\n",
      "2025-03-05 16:18:10,421 [INFO]   Val Loss: 114.124532 (Recon: 99.519296, KL: 14.605238)\n",
      "Epoch 45/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 70.73it/s, loss=324, recon=309, kl=15]    \n",
      "Epoch 45/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 34.50it/s, val_loss=143] \n",
      "2025-03-05 16:18:11,289 [INFO] Epoch 45/50 - Time: 0.87s\n",
      "2025-03-05 16:18:11,290 [INFO]   Train Loss: 100.072561 (Recon: 83.453998, KL: 16.618563)\n",
      "2025-03-05 16:18:11,291 [INFO]   Val Loss: 72.605594 (Recon: 51.084619, KL: 21.520975)\n",
      "Epoch 46/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 61.65it/s, loss=1.11e+3, recon=1.08e+3, kl=24.2]\n",
      "Epoch 46/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 33.95it/s, val_loss=90.5]\n",
      "2025-03-05 16:18:12,263 [INFO] Epoch 46/50 - Time: 0.97s\n",
      "2025-03-05 16:18:12,265 [INFO]   Train Loss: 98.750049 (Recon: 81.961313, KL: 16.788735)\n",
      "2025-03-05 16:18:12,266 [INFO]   Val Loss: 78.048726 (Recon: 62.270945, KL: 15.777778)\n",
      "Epoch 47/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 67.53it/s, loss=116, recon=99.7, kl=16.6] \n",
      "Epoch 47/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 30.49it/s, val_loss=145] \n",
      "2025-03-05 16:18:13,191 [INFO] Epoch 47/50 - Time: 0.92s\n",
      "2025-03-05 16:18:13,192 [INFO]   Train Loss: 92.901215 (Recon: 76.480154, KL: 16.421061)\n",
      "2025-03-05 16:18:13,195 [INFO]   Val Loss: 99.523499 (Recon: 81.470236, KL: 18.053264)\n",
      "Epoch 48/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 65.36it/s, loss=383, recon=365, kl=17.6]  \n",
      "Epoch 48/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 33.84it/s, val_loss=88]  \n",
      "2025-03-05 16:18:14,124 [INFO] Epoch 48/50 - Time: 0.93s\n",
      "2025-03-05 16:18:14,126 [INFO]   Train Loss: 88.762195 (Recon: 72.457439, KL: 16.304756)\n",
      "2025-03-05 16:18:14,127 [INFO]   Val Loss: 74.279651 (Recon: 56.082347, KL: 18.197304)\n",
      "Epoch 49/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 72.28it/s, loss=541, recon=520, kl=20.8]  \n",
      "Epoch 49/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 35.41it/s, val_loss=91.2]\n",
      "2025-03-05 16:18:14,976 [INFO] Epoch 49/50 - Time: 0.85s\n",
      "2025-03-05 16:18:14,978 [INFO]   Train Loss: 91.143351 (Recon: 75.086315, KL: 16.057037)\n",
      "2025-03-05 16:18:14,980 [INFO]   Val Loss: 69.213954 (Recon: 51.868243, KL: 17.345710)\n",
      "Epoch 50/50 [Train]: 100%|██████████| 48/48 [00:00<00:00, 70.07it/s, loss=453, recon=431, kl=21.8]  \n",
      "Epoch 50/50 [Val]: 100%|██████████| 6/6 [00:00<00:00, 27.57it/s, val_loss=110] \n",
      "2025-03-05 16:18:15,900 [INFO] Epoch 50/50 - Time: 0.92s\n",
      "2025-03-05 16:18:15,901 [INFO]   Train Loss: 97.436385 (Recon: 80.807487, KL: 16.628898)\n",
      "2025-03-05 16:18:15,904 [INFO]   Val Loss: 87.279726 (Recon: 70.490725, KL: 16.789001)\n",
      "2025-03-05 16:18:15,907 [INFO] Training complete - Total time: 58.10s\n",
      "2025-03-05 16:18:15,909 [INFO] Logging final model to WandB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/kl_loss</td><td>▁▃▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇████████████</td></tr><tr><td>train/loss</td><td>█▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/recon_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/kl_loss</td><td>▄▁▂▃▄▄▅▃▅▆▇▆▇▇▅▆█▅▅▄▅▅▇▆▅▆▇▇▇▇▇▆▆▅▆█▄▆▆▅</td></tr><tr><td>val/loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▂</td></tr><tr><td>val/recon_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▂▁▁▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/kl_loss</td><td>16.6289</td></tr><tr><td>train/loss</td><td>97.43638</td></tr><tr><td>train/recon_loss</td><td>80.80749</td></tr><tr><td>val/kl_loss</td><td>16.789</td></tr><tr><td>val/loss</td><td>87.27973</td></tr><tr><td>val/recon_loss</td><td>70.49073</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vae-fraud-training</strong> at: <a href='https://wandb.ai/alexkstern/fraud-vae/runs/6ho7ce2x' target=\"_blank\">https://wandb.ai/alexkstern/fraud-vae/runs/6ho7ce2x</a><br> View project at: <a href='https://wandb.ai/alexkstern/fraud-vae' target=\"_blank\">https://wandb.ai/alexkstern/fraud-vae</a><br>Synced 5 W&B file(s), 0 media file(s), 42 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250305_161714-6ho7ce2x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 6/6 [00:00<00:00, 29.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/loss: 65.655947\n",
      "test/recon_loss: 52.048761\n",
      "test/kl_loss: 13.607186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Minimal example for training VAE with your existing config file\n",
    "\n",
    "# Imports\n",
    "from dataloader import load_fraud_data, load_config\n",
    "from model_VAE import VariationalAutoEncoder\n",
    "from trainer import Trainer\n",
    "\n",
    "# Path to config file\n",
    "CONFIG_PATH = 'configs/fraud_vae/vae_test.config'\n",
    "\n",
    "try:\n",
    "    # 1. Load configuration and data\n",
    "    config = load_config(CONFIG_PATH)\n",
    "    data = load_fraud_data(config_path=CONFIG_PATH)\n",
    "\n",
    "    # 2. Create model from VAE section of config\n",
    "    model = VariationalAutoEncoder(config['VAE'])\n",
    "\n",
    "    # 3. Explicitly disable wandb if you don't want to use it\n",
    "    if 'WandB' not in config:\n",
    "        config['WandB'] = {}\n",
    "    config['WandB']['use_wandb'] = False  # Set to True if you want to use wandb\n",
    "\n",
    "    # 4. Create trainer and train\n",
    "    trainer = Trainer(model=model, dataloaders=data['dataloaders'], config=config)\n",
    "    history = trainer.train()\n",
    "\n",
    "    # 5. Evaluate on test set\n",
    "    test_metrics = trainer.evaluate_on_test()\n",
    "    print(\"\\nTest metrics:\")\n",
    "    for name, value in test_metrics.items():\n",
    "        print(f\"{name}: {value:.6f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # If any error occurs, make sure to clean up wandb\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    if 'trainer' in locals():\n",
    "        trainer.cleanup_wandb()\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    # Always ensure wandb is properly closed\n",
    "    if 'trainer' in locals():\n",
    "        trainer.cleanup_wandb()\n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Non-Fraud VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkstern/miniconda3/envs/credit_vae/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration from configs/non_fraud_vae/vae_test.config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12378/12378 [00:00<00:00, 22188.85 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset to class 0: 12000 samples\n",
      "Normalization statistics (calculated from class 0): {'Time': {'mean': 94364.65358333333, 'std': 47365.815157589255}, 'Amount': {'mean': 87.60478666666666, 'std': 240.59403081682598}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12378/12378 [00:00<00:00, 22458.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset to class 0: 12000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1547/1547 [00:00<00:00, 12679.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset to class 0: 1500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1548/1548 [00:00<00:00, 13912.50 examples/s]\n",
      "2025-03-05 16:33:23,255 [INFO] Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset to class 0: 1500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alexkstern/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexkstern\u001b[0m (\u001b[33malexksternteam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexkstern/fraudml/wandb/run-20250305_163327-ttxpm4pq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexkstern/non-fraud-vae/runs/ttxpm4pq' target=\"_blank\">vae-fraud-training</a></strong> to <a href='https://wandb.ai/alexkstern/non-fraud-vae' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexkstern/non-fraud-vae' target=\"_blank\">https://wandb.ai/alexkstern/non-fraud-vae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexkstern/non-fraud-vae/runs/ttxpm4pq' target=\"_blank\">https://wandb.ai/alexkstern/non-fraud-vae/runs/ttxpm4pq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 16:33:30,742 [INFO] WandB initialized: project=non-fraud-vae, run=vae-fraud-training\n",
      "2025-03-05 16:33:30,745 [INFO] Starting training\n",
      "Epoch 1/50 [Train]: 100%|██████████| 375/375 [00:04<00:00, 80.75it/s, loss=20.1, recon=13.9, kl=6.27]\n",
      "Epoch 1/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 89.69it/s, val_loss=21]   \n",
      "2025-03-05 16:33:35,936 [INFO] Saved checkpoint to saved_models/non_fraud/v1/VariationalAutoEncoder_non-fraud_20250305-163324_epoch1_val21.2505.pt\n",
      "2025-03-05 16:33:36,179 [INFO] Epoch 1/50 - Time: 5.43s\n",
      "2025-03-05 16:33:36,182 [INFO]   Train Loss: 25.413530 (Recon: 20.682736, KL: 4.730794)\n",
      "2025-03-05 16:33:36,185 [INFO]   Val Loss: 21.250500 (Recon: 14.804658, KL: 6.445842)\n",
      "Epoch 2/50 [Train]: 100%|██████████| 375/375 [00:04<00:00, 80.13it/s, loss=19.9, recon=12.8, kl=7.13]\n",
      "Epoch 2/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 83.15it/s, val_loss=19.5] \n",
      "2025-03-05 16:33:41,455 [INFO] Saved checkpoint to saved_models/non_fraud/v1/VariationalAutoEncoder_non-fraud_20250305-163324_epoch2_val20.2577.pt\n",
      "2025-03-05 16:33:41,638 [INFO] Epoch 2/50 - Time: 5.45s\n",
      "2025-03-05 16:33:41,640 [INFO]   Train Loss: 21.398632 (Recon: 15.004497, KL: 6.394135)\n",
      "2025-03-05 16:33:41,641 [INFO]   Val Loss: 20.257696 (Recon: 13.491593, KL: 6.766103)\n",
      "Epoch 3/50 [Train]: 100%|██████████| 375/375 [00:04<00:00, 77.52it/s, loss=18, recon=11.7, kl=6.27]  \n",
      "Epoch 3/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 87.85it/s, val_loss=18.8]\n",
      "2025-03-05 16:33:47,036 [INFO] Saved checkpoint to saved_models/non_fraud/v1/VariationalAutoEncoder_non-fraud_20250305-163324_epoch3_val18.8387.pt\n",
      "2025-03-05 16:33:47,216 [INFO] Epoch 3/50 - Time: 5.57s\n",
      "2025-03-05 16:33:47,219 [INFO]   Train Loss: 20.543530 (Recon: 13.816684, KL: 6.726845)\n",
      "2025-03-05 16:33:47,221 [INFO]   Val Loss: 18.838723 (Recon: 12.155888, KL: 6.682835)\n",
      "Epoch 4/50 [Train]: 100%|██████████| 375/375 [00:04<00:00, 77.18it/s, loss=21.2, recon=14.9, kl=6.32]\n",
      "Epoch 4/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 95.19it/s, val_loss=18.4] \n",
      "2025-03-05 16:33:52,597 [INFO] Saved checkpoint to saved_models/non_fraud/v1/VariationalAutoEncoder_non-fraud_20250305-163324_epoch4_val18.6766.pt\n",
      "2025-03-05 16:33:52,768 [INFO] Epoch 4/50 - Time: 5.54s\n",
      "2025-03-05 16:33:52,770 [INFO]   Train Loss: 19.856810 (Recon: 13.077726, KL: 6.779084)\n",
      "2025-03-05 16:33:52,772 [INFO]   Val Loss: 18.676558 (Recon: 11.215513, KL: 7.461045)\n",
      "Epoch 5/50 [Train]: 100%|██████████| 375/375 [00:04<00:00, 77.08it/s, loss=17.7, recon=11, kl=6.73]  \n",
      "Epoch 5/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 78.41it/s, val_loss=18.3]\n",
      "2025-03-05 16:33:58,261 [INFO] Saved checkpoint to saved_models/non_fraud/v1/VariationalAutoEncoder_non-fraud_20250305-163324_epoch5_val18.5573.pt\n",
      "2025-03-05 16:33:58,454 [INFO] Epoch 5/50 - Time: 5.68s\n",
      "2025-03-05 16:33:58,458 [INFO]   Train Loss: 19.514512 (Recon: 12.677542, KL: 6.836970)\n",
      "2025-03-05 16:33:58,460 [INFO]   Val Loss: 18.557262 (Recon: 11.194424, KL: 7.362837)\n",
      "Epoch 6/50 [Train]: 100%|██████████| 375/375 [00:05<00:00, 68.59it/s, loss=18.8, recon=11.9, kl=6.85]\n",
      "Epoch 6/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 79.81it/s, val_loss=16.6]\n",
      "2025-03-05 16:34:04,540 [INFO] Saved checkpoint to saved_models/non_fraud/v1/VariationalAutoEncoder_non-fraud_20250305-163324_epoch6_val18.0169.pt\n",
      "2025-03-05 16:34:04,721 [INFO] Epoch 6/50 - Time: 6.26s\n",
      "2025-03-05 16:34:04,723 [INFO]   Train Loss: 19.253359 (Recon: 12.428249, KL: 6.825110)\n",
      "2025-03-05 16:34:04,725 [INFO]   Val Loss: 18.016911 (Recon: 10.885618, KL: 7.131292)\n",
      "Epoch 7/50 [Train]: 100%|██████████| 375/375 [00:05<00:00, 66.58it/s, loss=16.9, recon=10.4, kl=6.55]\n",
      "Epoch 7/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 84.88it/s, val_loss=18.2] \n",
      "2025-03-05 16:34:10,928 [INFO] Epoch 7/50 - Time: 6.20s\n",
      "2025-03-05 16:34:10,930 [INFO]   Train Loss: 19.016949 (Recon: 12.158642, KL: 6.858307)\n",
      "2025-03-05 16:34:10,931 [INFO]   Val Loss: 18.577778 (Recon: 10.775307, KL: 7.802471)\n",
      "Epoch 8/50 [Train]: 100%|██████████| 375/375 [00:04<00:00, 75.49it/s, loss=16.7, recon=9.76, kl=6.92]\n",
      "Epoch 8/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 74.44it/s, val_loss=18]  \n",
      "2025-03-05 16:34:16,547 [INFO] Epoch 8/50 - Time: 5.61s\n",
      "2025-03-05 16:34:16,549 [INFO]   Train Loss: 18.747686 (Recon: 11.832619, KL: 6.915067)\n",
      "2025-03-05 16:34:16,552 [INFO]   Val Loss: 18.477192 (Recon: 10.865731, KL: 7.611460)\n",
      "Epoch 9/50 [Train]: 100%|██████████| 375/375 [00:04<00:00, 75.48it/s, loss=15.5, recon=9.08, kl=6.41]\n",
      "Epoch 9/50 [Val]: 100%|██████████| 47/47 [00:00<00:00, 90.86it/s, val_loss=17.7] \n",
      "2025-03-05 16:34:22,056 [INFO] Early stopping triggered after 9 epochs\n",
      "2025-03-05 16:34:22,057 [INFO] Training complete - Total time: 51.31s\n",
      "2025-03-05 16:34:22,059 [INFO] Logging final model to WandB\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/kl_loss</td><td>▁▆▇█████</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▁▁</td></tr><tr><td>train/recon_loss</td><td>█▄▃▂▂▁▁▁</td></tr><tr><td>val/kl_loss</td><td>▁▃▂▆▆▅█▇</td></tr><tr><td>val/loss</td><td>█▆▃▂▂▁▂▂</td></tr><tr><td>val/recon_loss</td><td>█▆▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/kl_loss</td><td>6.91507</td></tr><tr><td>train/loss</td><td>18.74769</td></tr><tr><td>train/recon_loss</td><td>11.83262</td></tr><tr><td>val/kl_loss</td><td>7.61146</td></tr><tr><td>val/loss</td><td>18.47719</td></tr><tr><td>val/recon_loss</td><td>10.86573</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vae-fraud-training</strong> at: <a href='https://wandb.ai/alexkstern/non-fraud-vae/runs/ttxpm4pq' target=\"_blank\">https://wandb.ai/alexkstern/non-fraud-vae/runs/ttxpm4pq</a><br> View project at: <a href='https://wandb.ai/alexkstern/non-fraud-vae' target=\"_blank\">https://wandb.ai/alexkstern/non-fraud-vae</a><br>Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250305_163327-ttxpm4pq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   2%|▏         | 1/47 [00:00<00:14,  3.19it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m history \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 4. Evaluate on test set\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_on_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m test_metrics\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/credit_vae/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fraudml/trainer.py:440\u001b[0m, in \u001b[0;36mTrainer.evaluate_on_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m recon_batch, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/credit_vae/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/credit_vae/lib/python3.8/site-packages/torch/nn/modules/module.py:1616\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1616\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/credit_vae/lib/python3.8/site-packages/wandb/integration/torch/wandb_torch.py:113\u001b[0m, in \u001b[0;36mTorchHistory.add_log_parameters_hook.<locals>.<lambda>\u001b[0;34m(mod, inp, outp)\u001b[0m\n\u001b[1;32m    110\u001b[0m log_track_params \u001b[38;5;241m=\u001b[39m log_track_init(log_freq)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     hook \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mregister_forward_hook(\n\u001b[0;32m--> 113\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m mod, inp, outp: \u001b[43mparameter_log_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_track_params\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_handles[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix] \u001b[38;5;241m=\u001b[39m hook\n\u001b[1;32m    118\u001b[0m     module\u001b[38;5;241m.\u001b[39m_wandb_hook_names\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prefix)\n",
      "File \u001b[0;32m~/miniconda3/envs/credit_vae/lib/python3.8/site-packages/wandb/integration/torch/wandb_torch.py:108\u001b[0m, in \u001b[0;36mTorchHistory.add_log_parameters_hook.<locals>.parameter_log_hook\u001b[0;34m(module, input_, output, log_track)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     data \u001b[38;5;241m=\u001b[39m parameter\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_tensor_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/credit_vae/lib/python3.8/site-packages/wandb/integration/torch/wandb_torch.py:254\u001b[0m, in \u001b[0;36mTorchHistory.log_tensor_stats\u001b[0;34m(self, tensor, name)\u001b[0m\n\u001b[1;32m    251\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(tensor_np)\n\u001b[1;32m    252\u001b[0m     bins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(bins_np)\n\u001b[0;32m--> 254\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m(\n\u001b[1;32m    255\u001b[0m     {name: wandb\u001b[38;5;241m.\u001b[39mHistogram(np_histogram\u001b[38;5;241m=\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mtolist(), bins\u001b[38;5;241m.\u001b[39mtolist()))},\n\u001b[1;32m    256\u001b[0m     commit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    257\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_log'"
     ]
    }
   ],
   "source": [
    "# Minimal example for training VAE with your existing config file\n",
    "\n",
    "# Imports\n",
    "from dataloader import load_fraud_data, load_config\n",
    "from model_VAE import VariationalAutoEncoder\n",
    "from trainer import Trainer\n",
    "\n",
    "# Path to config file\n",
    "CONFIG_PATH = 'configs/non_fraud_vae/vae_test.config'\n",
    "\n",
    "try:\n",
    "    # 1. Load configuration and data\n",
    "    config = load_config(CONFIG_PATH)\n",
    "    data = load_fraud_data(config_path=CONFIG_PATH)\n",
    "\n",
    "    # 2. Create model from VAE section of config\n",
    "    model = VariationalAutoEncoder(config['VAE'])\n",
    "\n",
    "    # 3. Explicitly disable wandb if you don't want to use it\n",
    "    if 'WandB' not in config:\n",
    "        config['WandB'] = {}\n",
    "    config['WandB']['use_wandb'] = False  # Set to True if you want to use wandb\n",
    "\n",
    "    # 4. Create trainer and train\n",
    "    trainer = Trainer(model=model, dataloaders=data['dataloaders'], config=config)\n",
    "    history = trainer.train()\n",
    "\n",
    "    # 5. Evaluate on test set\n",
    "    test_metrics = trainer.evaluate_on_test()\n",
    "    print(\"\\nTest metrics:\")\n",
    "    for name, value in test_metrics.items():\n",
    "        print(f\"{name}: {value:.6f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # If any error occurs, make sure to clean up wandb\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    if 'trainer' in locals():\n",
    "        trainer.cleanup_wandb()\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    # Always ensure wandb is properly closed\n",
    "    if 'trainer' in locals():\n",
    "        trainer.cleanup_wandb()\n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classifier with trained latent Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset...\n",
      "DataFrame shape: (12378, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_latent_0</th>\n",
       "      <th>fraud_latent_1</th>\n",
       "      <th>fraud_latent_2</th>\n",
       "      <th>fraud_latent_3</th>\n",
       "      <th>fraud_latent_4</th>\n",
       "      <th>fraud_latent_5</th>\n",
       "      <th>fraud_latent_6</th>\n",
       "      <th>fraud_latent_7</th>\n",
       "      <th>fraud_latent_8</th>\n",
       "      <th>fraud_latent_9</th>\n",
       "      <th>...</th>\n",
       "      <th>non_fraud_latent_2</th>\n",
       "      <th>non_fraud_latent_3</th>\n",
       "      <th>non_fraud_latent_4</th>\n",
       "      <th>non_fraud_latent_5</th>\n",
       "      <th>non_fraud_latent_6</th>\n",
       "      <th>non_fraud_latent_7</th>\n",
       "      <th>non_fraud_latent_8</th>\n",
       "      <th>non_fraud_latent_9</th>\n",
       "      <th>non_fraud_recon_error</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.615643</td>\n",
       "      <td>0.751735</td>\n",
       "      <td>-0.404673</td>\n",
       "      <td>0.545401</td>\n",
       "      <td>-0.288656</td>\n",
       "      <td>-0.766497</td>\n",
       "      <td>0.099406</td>\n",
       "      <td>2.070422</td>\n",
       "      <td>0.142465</td>\n",
       "      <td>-0.773488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169732</td>\n",
       "      <td>-0.540064</td>\n",
       "      <td>0.177553</td>\n",
       "      <td>-0.771990</td>\n",
       "      <td>-1.690295</td>\n",
       "      <td>-1.534132</td>\n",
       "      <td>-2.203338</td>\n",
       "      <td>0.374561</td>\n",
       "      <td>0.354833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.250439</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.025312</td>\n",
       "      <td>-0.269650</td>\n",
       "      <td>-0.987701</td>\n",
       "      <td>-0.920562</td>\n",
       "      <td>-0.994900</td>\n",
       "      <td>1.402146</td>\n",
       "      <td>-0.038242</td>\n",
       "      <td>0.495690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.518433</td>\n",
       "      <td>-0.979053</td>\n",
       "      <td>1.128486</td>\n",
       "      <td>-0.038908</td>\n",
       "      <td>0.519204</td>\n",
       "      <td>0.301190</td>\n",
       "      <td>0.725730</td>\n",
       "      <td>-0.704731</td>\n",
       "      <td>0.105169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.041233</td>\n",
       "      <td>1.005673</td>\n",
       "      <td>-0.295047</td>\n",
       "      <td>-0.495441</td>\n",
       "      <td>-0.633346</td>\n",
       "      <td>-0.538213</td>\n",
       "      <td>-0.912619</td>\n",
       "      <td>1.734814</td>\n",
       "      <td>0.155106</td>\n",
       "      <td>-0.067113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.645789</td>\n",
       "      <td>1.043757</td>\n",
       "      <td>-0.616265</td>\n",
       "      <td>0.510356</td>\n",
       "      <td>-0.252283</td>\n",
       "      <td>-0.158109</td>\n",
       "      <td>-0.577012</td>\n",
       "      <td>0.256973</td>\n",
       "      <td>0.280511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.411137</td>\n",
       "      <td>0.593400</td>\n",
       "      <td>-0.428755</td>\n",
       "      <td>0.556435</td>\n",
       "      <td>-0.720047</td>\n",
       "      <td>-0.651324</td>\n",
       "      <td>-0.310020</td>\n",
       "      <td>2.148195</td>\n",
       "      <td>-0.593145</td>\n",
       "      <td>-0.470835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.988748</td>\n",
       "      <td>-0.495142</td>\n",
       "      <td>0.539933</td>\n",
       "      <td>-0.649971</td>\n",
       "      <td>0.252956</td>\n",
       "      <td>1.011837</td>\n",
       "      <td>-0.883104</td>\n",
       "      <td>1.754187</td>\n",
       "      <td>0.570871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.627044</td>\n",
       "      <td>0.368182</td>\n",
       "      <td>-0.246027</td>\n",
       "      <td>-0.848488</td>\n",
       "      <td>-0.669438</td>\n",
       "      <td>-0.823566</td>\n",
       "      <td>-0.687639</td>\n",
       "      <td>2.041727</td>\n",
       "      <td>0.406208</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859374</td>\n",
       "      <td>0.759472</td>\n",
       "      <td>-0.138708</td>\n",
       "      <td>0.008946</td>\n",
       "      <td>-0.660882</td>\n",
       "      <td>-1.112314</td>\n",
       "      <td>-0.496360</td>\n",
       "      <td>-0.240784</td>\n",
       "      <td>0.230561</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fraud_latent_0  fraud_latent_1  fraud_latent_2  fraud_latent_3  \\\n",
       "0       -0.615643        0.751735       -0.404673        0.545401   \n",
       "1       -1.250439        0.235100        0.025312       -0.269650   \n",
       "2       -1.041233        1.005673       -0.295047       -0.495441   \n",
       "3       -1.411137        0.593400       -0.428755        0.556435   \n",
       "4       -0.627044        0.368182       -0.246027       -0.848488   \n",
       "\n",
       "   fraud_latent_4  fraud_latent_5  fraud_latent_6  fraud_latent_7  \\\n",
       "0       -0.288656       -0.766497        0.099406        2.070422   \n",
       "1       -0.987701       -0.920562       -0.994900        1.402146   \n",
       "2       -0.633346       -0.538213       -0.912619        1.734814   \n",
       "3       -0.720047       -0.651324       -0.310020        2.148195   \n",
       "4       -0.669438       -0.823566       -0.687639        2.041727   \n",
       "\n",
       "   fraud_latent_8  fraud_latent_9  ...  non_fraud_latent_2  \\\n",
       "0        0.142465       -0.773488  ...            0.169732   \n",
       "1       -0.038242        0.495690  ...           -0.518433   \n",
       "2        0.155106       -0.067113  ...           -0.645789   \n",
       "3       -0.593145       -0.470835  ...           -0.988748   \n",
       "4        0.406208       -0.008551  ...            0.859374   \n",
       "\n",
       "   non_fraud_latent_3  non_fraud_latent_4  non_fraud_latent_5  \\\n",
       "0           -0.540064            0.177553           -0.771990   \n",
       "1           -0.979053            1.128486           -0.038908   \n",
       "2            1.043757           -0.616265            0.510356   \n",
       "3           -0.495142            0.539933           -0.649971   \n",
       "4            0.759472           -0.138708            0.008946   \n",
       "\n",
       "   non_fraud_latent_6  non_fraud_latent_7  non_fraud_latent_8  \\\n",
       "0           -1.690295           -1.534132           -2.203338   \n",
       "1            0.519204            0.301190            0.725730   \n",
       "2           -0.252283           -0.158109           -0.577012   \n",
       "3            0.252956            1.011837           -0.883104   \n",
       "4           -0.660882           -1.112314           -0.496360   \n",
       "\n",
       "   non_fraud_latent_9  non_fraud_recon_error  label  \n",
       "0            0.374561               0.354833      0  \n",
       "1           -0.704731               0.105169      0  \n",
       "2            0.256973               0.280511      0  \n",
       "3            1.754187               0.570871      0  \n",
       "4           -0.240784               0.230561      0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple script to load VAE dataset without the original data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory with the VAE dataset\n",
    "data_dir = 'vae_dataset'\n",
    "\n",
    "# Select which split to visualize\n",
    "split = 'train'  # Options: 'train', 'val', 'test'\n",
    "\n",
    "# Load the data\n",
    "print(f\"Loading {split} dataset...\")\n",
    "X = np.load(os.path.join(data_dir, f'{split}_combined_features.npy'))\n",
    "feature_names = np.load(os.path.join(data_dir, f'{split}_feature_names.npy'), allow_pickle=True)\n",
    "labels = np.load(os.path.join(data_dir, f'{split}_labels.npy'))\n",
    "\n",
    "# Create DataFrame with feature names\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Add label column\n",
    "df['label'] = labels\n",
    "\n",
    "# Display basic information\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "\n",
    "# Display the head of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset...\n",
      "Loading test dataset...\n",
      "Train VAE features shape: (12378, 22)\n",
      "Train original features shape: (12378, 30)\n",
      "Train labels shape: (12378,)\n",
      "Test VAE features shape: (1548, 22)\n",
      "Test original features shape: (1548, 30)\n",
      "Test labels shape: (1548,)\n",
      "\n",
      "Training MLP on VAE features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkstern/miniconda3/envs/credit_vae/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 18.07 seconds\n",
      "\n",
      "Training MLP on original features...\n",
      "Training completed in 2.61 seconds\n",
      "\n",
      "Evaluating MLP with VAE features on test set...\n",
      "Accuracy (VAE features): 0.9432\n",
      "ROC AUC (VAE features): 0.2749\n",
      "Classification Report (VAE features):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      1500\n",
      "           1       0.10      0.10      0.10        48\n",
      "\n",
      "    accuracy                           0.94      1548\n",
      "   macro avg       0.54      0.54      0.54      1548\n",
      "weighted avg       0.94      0.94      0.94      1548\n",
      "\n",
      "\n",
      "Evaluating MLP with original features on test set...\n",
      "Accuracy (original features): 0.9690\n",
      "ROC AUC (original features): 0.6034\n",
      "Classification Report (original features):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1500\n",
      "           1       0.00      0.00      0.00        48\n",
      "\n",
      "    accuracy                           0.97      1548\n",
      "   macro avg       0.48      0.50      0.49      1548\n",
      "weighted avg       0.94      0.97      0.95      1548\n",
      "\n",
      "\n",
      "=== Performance Comparison ===\n",
      "              Metric  VAE Features  Original Features\n",
      "0           Accuracy      0.943152           0.968992\n",
      "1            ROC AUC      0.274938           0.603417\n",
      "2  Training Time (s)     18.066717           2.607410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkstern/miniconda3/envs/credit_vae/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alexkstern/miniconda3/envs/credit_vae/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/alexkstern/miniconda3/envs/credit_vae/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIvElEQVR4nO3deZyO9f4/8PcYzMieJZQtCpU2+qqcQkT74pTTdkKb036SFqfTES1Oq1a0p6JVe0qLtDha0SpRtKIQCllmrt8f/UzdzWCIayzP5+Mxj0f359re9zLj3ev+XNeVlSRJEgAAAACQolIlXQAAAAAAmx6hFAAAAACpE0oBAAAAkDqhFAAAAACpE0oBAAAAkDqhFAAAAACpE0oBAAAAkDqhFAAAAACpE0oBAAAAkDqhFJSAyZMnR8eOHaNy5cqRlZUVTzzxxFrd/7Rp0yIrKyvuueeetbrfDVnbtm2jbdu2JV0GAGyy9D/p25j6n6ysrLjkkkvWaNsGDRpEt27d1mo9f9StW7do0KDBOj0GbIyEUmyyPv/88+jRo0dsvfXWkZubG5UqVYrWrVvHDTfcEIsWLVqnx+7atWt8+OGHcfnll8d9990XLVu2XKfHS1O3bt0iKysrKlWqVOTrOHny5MjKyoqsrKy45pprVnv/3333XVxyySUxYcKEtVDtujVu3LjIysqKf//73ytcZ/nr0bNnz4zx888/P7KysuJvf/tbkdstb7xX9PPf//53pbVdcsklK9x28ODBq/9ki2HEiBFr3EwCsHbof9YN/U/Rli5dGjfeeGPstttuUbFixahQoULstttuceONN8bSpUtLurwS0bZt2xX2YJ9++uk6OebAgQOFtay3Spd0AVASnn322TjyyCMjJycnjj/++Nhhhx1iyZIl8cYbb8R5550XH3/8cdx2223r5NiLFi2KsWPHxkUXXRRnnHHGOjlG/fr1Y9GiRVGmTJl1sv9VKV26dCxcuDCefvrp6NKlS8ayoUOHRm5ubvzyyy9rtO/vvvsu+vbtGw0aNIidd9652Nu98MILa3S8P2PXXXeNpk2bxgMPPBCXXXZZkesMGzYsIiKOO+64grEkSeKBBx6IBg0axNNPPx0//fRTVKxYscjtjz766DjggAMKje+yyy7FqnHQoEFRoUKFjLFWrVoVa9vVNWLEiLjlllsEUwAlRP+zbul/Mi1YsCAOPPDAePXVV+Oggw6Kbt26RalSpeL555+Ps88+Ox577LF49tlno3z58sXa36JFi6J06TX739dJkyZFqVLrz3yMrbbaKvr3719ovE6dOuvkeAMHDozq1auv89lisCaEUmxypk6dGkcddVTUr18/Ro0aFbVr1y5Ydvrpp8eUKVPi2WefXWfH/+GHHyIiokqVKuvsGFlZWZGbm7vO9r8qOTk50bp163jggQcKNWXDhg2LAw88MIYPH55KLQsXLozNNtssypYtm8rx/ujYY4+Niy++ON58883YfffdCy1/4IEHomnTprHrrrsWjI0ePTq++eabGDVqVHTq1Ckee+yx6Nq1a5H733XXXTMCrdV1xBFHRPXq1dd4+/XBggULit3QAmyq9D/rnv4nU8+ePePVV1+Nm266KSOIPPXUU+OWW26JM844I3r16hWDBg1a4T7y8/NjyZIlkZub+6fe25ycnDXedl2oXLnyn+rf1gdJksQvv/wS5cqVK+lS2MCtP3ExpOSqq66Kn3/+Oe68886Mhmy5xo0bx9lnn13weNmyZXHppZdGo0aNIicnJxo0aBD/+te/YvHixRnbNWjQIA466KB444034v/+7/8iNzc3tt5667j33nsL1rnkkkuifv36ERFx3nnnRVZWVsG55ys6D335aVa/9+KLL8Zf/vKXqFKlSlSoUCGaNGkS//rXvwqWr+iaCqNGjYq99torypcvH1WqVIlDDz00Jk6cWOTxpkyZEt26dYsqVapE5cqVo3v37rFw4cIVv7B/cMwxx8Rzzz0Xc+fOLRh75513YvLkyXHMMccUWn/OnDnRq1evaN68eVSoUCEqVaoU+++/f7z//vsF64wePTp22223iIjo3r17wVTn5c+zbdu2scMOO8R7770Xe++9d2y22WYFr8sfr6nQtWvXyM3NLfT8O3XqFFWrVo3vvvuu2M91ZY499tiI+G1G1O+99957MWnSpIJ1lhs6dGhst9120a5du+jQoUMMHTp0rdSyJu6///5o0aJFlCtXLjbffPM46qij4uuvv85Y5/XXX48jjzwy6tWrFzk5OVG3bt0455xzMk5f6NatW9xyyy0RERnT1CN+fV+zsrJi9OjRGfst6nPcrVu3qFChQnz++edxwAEHRMWKFQtev/z8/Lj++utj++23j9zc3Nhiiy2iR48e8eOPP2bs9913341OnTpF9erVo1y5ctGwYcM44YQT1tZLBrBe0v/ofyLS63+++eabuPPOO2OfffYpcmbc6aefHu3atYs77rgjvvnmm4LxrKysOOOMM2Lo0KGx/fbbR05OTjz//PMFy/4423r06NHRsmXLyM3NjUaNGsWtt95a5Gfnj9eUuueeeyIrKyvGjBkTPXv2jBo1akT58uXj8MMPLwhQl3vyySfjwAMPjDp16kROTk40atQoLr300sjLy/uTr9KKLV68OPr06RONGzcu6K3OP//8Qr9/d999d+yzzz5Rs2bNyMnJie22265QyNegQYP4+OOP49VXXy347Cz/TBT1WkX89vpMmzYtYz8HHXRQjBw5Mlq2bBnlypWLW2+9NSIi5s6dG//85z+jbt26kZOTE40bN44rr7wy8vPzM/b74IMPRosWLaJixYpRqVKlaN68edxwww1r4RVjQ2amFJucp59+OrbeeuvYc889i7X+SSedFEOGDIkjjjgizj333Hjrrbeif//+MXHixHj88ccz1p0yZUocccQRceKJJ0bXrl3jrrvuim7dukWLFi1i++23j86dO0eVKlXinHPOKTjt6o+nTq3Kxx9/HAcddFDsuOOO0a9fv8jJyYkpU6bEmDFjVrrdSy+9FPvvv39svfXWcckll8SiRYvipptuitatW8e4ceMKNYRdunSJhg0bRv/+/WPcuHFxxx13RM2aNePKK68sVp2dO3eOf/zjH/HYY48V/A//sGHDCs0KWu6LL76IJ554Io488sho2LBhzJw5M2699dZo06ZNfPLJJ1GnTp1o1qxZ9OvXL/7zn//EKaecEnvttVdERMZ7OXv27Nh///3jqKOOiuOOOy622GKLIuu74YYbYtSoUdG1a9cYO3ZsZGdnx6233hovvPBC3HfffWtt+nTDhg1jzz33jIcffjgGDBgQ2dnZBcuWB1W/b1IXL14cw4cPj3PPPTcifj09r3v37jFjxoyoVatWof0vXLgwZs2aVWi8SpUqxZriPmfOnIzH2dnZUbVq1YiIuPzyy+Piiy+OLl26xEknnRQ//PBD3HTTTbH33nvH+PHjC77tfuSRR2LhwoVx6qmnRrVq1eLtt9+Om266Kb755pt45JFHIiKiR48e8d1338WLL74Y99133yrrWplly5ZFp06d4i9/+Utcc801sdlmmxUc45577onu3bvHWWedFVOnTo2bb745xo8fH2PGjIkyZcrE999/Hx07dowaNWrEhRdeGFWqVIlp06bFY4899qdqAljf6X/0PxHp9T/PPfdc5OXlxfHHH7/CdY4//vh45ZVX4vnnn4+TTjqpYHzUqFHx8MMPxxlnnBHVq1df4cXDx48fH/vtt1/Url07+vbtG3l5edGvX7+oUaNGses888wzo2rVqtGnT5+YNm1aXH/99XHGGWfEQw89VLDOPffcExUqVIiePXtGhQoVYtSoUfGf//wn5s+fH1dffXWxj/V7eXl5hfq33NzcqFChQuTn58chhxwSb7zxRpxyyinRrFmz+PDDD2PAgAHx2WefZdwgYNCgQbH99tvHIYccEqVLl46nn346TjvttMjPz4/TTz89IiKuv/76OPPMM6NChQpx0UUXRUSs8POxKpMmTYqjjz46evToESeffHI0adIkFi5cGG3atIlvv/02evToEfXq1Yv//e9/0bt375g+fXpcf/31EfFrqHz00UdH+/btC36fJk6cGGPGjMkIxNkEJbAJmTdvXhIRyaGHHlqs9SdMmJBERHLSSSdljPfq1SuJiGTUqFEFY/Xr108iInnttdcKxr7//vskJycnOffccwvGpk6dmkREcvXVV2fss2vXrkn9+vUL1dCnT5/k97+qAwYMSCIi+eGHH1ZY9/Jj3H333QVjO++8c1KzZs1k9uzZBWPvv/9+UqpUqeT4448vdLwTTjghY5+HH354Uq1atRUe8/fPo3z58kmSJMkRRxyRtG/fPkmSJMnLy0tq1aqV9O3bt8jX4Jdffkny8vIKPY+cnJykX79+BWPvvPNOoee2XJs2bZKISAYPHlzksjZt2mSMjRw5MomI5LLLLku++OKLpEKFCslhhx22yue4um655ZYkIpKRI0cWjOXl5SVbbrllsscee2Ss++ijjyYRkUyePDlJkiSZP39+kpubmwwYMCBjveWv4Yp+xo4du9Kalr/Pf/xZ/hmcNm1akp2dnVx++eUZ23344YdJ6dKlM8YXLlxYaP/9+/dPsrKyki+//LJg7PTTT0+K+mfnlVdeSSIieeWVV4p8jr9/r7t27ZpERHLhhRdmrPv6668nEZEMHTo0Y/z555/PGH/88ceTiEjeeeedFb84ABsZ/Y/+5/fS6H/++c9/JhGRjB8/foXrjBs3LomIpGfPngVjEZGUKlUq+fjjjwutHxFJnz59Ch4ffPDByWabbZZ8++23BWOTJ09OSpcuXajfqF+/ftK1a9eCx3fffXcSEUmHDh2S/Pz8gvFzzjknyc7OTubOnVswVlSf06NHj2SzzTZLfvnll4KxFX2W/2j5+/XHn+X13XfffUmpUqWS119/PWO7wYMHJxGRjBkzZqW1derUKdl6660zxrbffvtCn4MkKfx7ttzy12fq1KkFY8t/159//vmMdS+99NKkfPnyyWeffZYxfuGFFybZ2dnJV199lSRJkpx99tlJpUqVkmXLlhV+UdikOX2PTcr8+fMjIlZ40eg/GjFiREREoTujLZ/F8sdrL2y33XYF315FRNSoUSOaNGkSX3zxxRrX/EfLZ6c8+eSThabErsj06dNjwoQJ0a1bt9h8880LxnfcccfYd999C57n7/3jH//IeLzXXnvF7NmzC17D4jjmmGNi9OjRMWPGjBg1alTMmDGjyKnrEb+e67/8ApR5eXkxe/bsgqn548aNK/Yxc3Jyonv37sVat2PHjtGjR4/o169fdO7cOXJzcwumIa9Nf/vb36JMmTIZp/C9+uqr8e233xZ56l7Lli2jcePGEfHrZ/XAAw9c4Sl8p5xySrz44ouFfrbbbrti1TZ8+PCM7ZYf57HHHov8/Pzo0qVLzJo1q+CnVq1asc0228Qrr7xSsI/fX0tgwYIFMWvWrNhzzz0jSZIYP3588V6k1XTqqadmPH7kkUeicuXKse+++2bU26JFi6hQoUJBvct/f5555plN9q4/wKZH/6P/+b00+p+ffvopIlb+mVu+7I+vbZs2bVbZx+Tl5cVLL70Uhx12WMbsrsaNG8f+++9f7DpPOeWUjNPX9tprr8jLy4svv/yyYOz3fc5PP/0Us2bNir322isWLly4xnfLa9CgQaHe7fzzz4+IX3uaZs2aRdOmTTN6mn322SciYoU92Lx582LWrFnRpk2b+OKLL2LevHlrVNvKNGzYMDp16pQx9sgjj8Ree+0VVatWzai3Q4cOkZeXF6+99lpE/Po7vGDBgnjxxRfXel1s2Jy+xyalUqVKEfHbP5Sr8uWXX0apUqUKAoLlatWqFVWqVMn4Bysiol69eoX2UbVq1ULXtPkz/va3v8Udd9wRJ510Ulx44YXRvn376Ny5cxxxxBErvKvI8jqbNGlSaFmzZs1i5MiRhS4W/cfnsvyUrh9//LHgdVyV5df8eeihh2LChAmx2267RePGjTPOT18uPz8/brjhhhg4cGBMnTo14zz9atWqFet4ERFbbrnlal3U85prroknn3wyJkyYEMOGDYuaNWuucpsffvgho74KFSqs9DSEatWqRadOneLxxx+PwYMHR25ubgwbNixKly6dcSHUuXPnxogRI+KMM86IKVOmFIy3bt06hg8fHp999llsu+22GfveZpttokOHDsV+vn+09957F3mh88mTJ0eSJLHNNtsUud3v72z01VdfxX/+85946qmnCn3W10VDVLp06dhqq60K1Ttv3rwVvn/ff/99RPza6P71r3+Nvn37xoABA6Jt27Zx2GGHxTHHHLPeXQQVYG3R/+h//mhd9z/LA6eVfeZWFFw1bNhwlbV8//33sWjRokKf0YgocmxFVvZ+L/fxxx/Hv//97xg1alShAG1N+5zy5cuvsH+bPHlyTJw4cYWnIS7vaSIixowZE3369ImxY8cWuvbZvHnzonLlymtU34oU9d5Mnjw5Pvjgg1XWe9ppp8XDDz8c+++/f2y55ZbRsWPH6NKlS+y3335rtUY2PEIpNimVKlWKOnXqxEcffbRa2xV1AcCi/P56Qb+XJMkaH+OPF1EsV65cvPbaa/HKK6/Es88+G88//3w89NBDsc8++8QLL7ywwhpW1595Lsvl5ORE586dY8iQIfHFF18Uujjl711xxRVx8cUXxwknnBCXXnppbL755lGqVKn45z//WexvRCNite8AMn78+IJ/LD/88MM4+uijV7nNbrvtltGQ9+nTZ6XPLSLiuOOOi2eeeSaeeeaZOOSQQ2L48OEF1zZa7pFHHonFixfHtddeG9dee22hfQwdOjT69u1bzGf25+Tn50dWVlY899xzRX4WljeheXl5se+++8acOXPiggsuiKZNm0b58uXj22+/jW7duhXrvSvuZ3+533+r/Pt6a9asucIZZctf56ysrHj00UfjzTffjKeffjpGjhwZJ5xwQlx77bXx5ptvrvY1TgA2BPqf4tP/rNjq9D/NmjWLiIgPPvggdt555yLX+eCDDyIiCs2KSvNubqt6v+fOnRtt2rSJSpUqRb9+/aJRo0aRm5sb48aNiwsuuGC13qPiys/Pj+bNm8d1111X5PK6detGRMTnn38e7du3j6ZNm8Z1110XdevWjbJly8aIESNiwIAB66QHK+q9yc/Pj3333bdgptcfLf9CtWbNmjFhwoQYOXJkPPfcc/Hcc8/F3XffHccff3wMGTJklbWy8RJKsck56KCD4rbbbouxY8fGHnvssdJ169evH/n5+TF58uSCf1wjImbOnBlz584tuJPM2lC1atWMO7Us98dvIyMiSpUqFe3bt4/27dvHddddF1dccUVcdNFF8corrxT5rcvyOidNmlRo2aeffhrVq1fP+JZwbTrmmGPirrvuilKlSsVRRx21wvUeffTRaNeuXdx5550Z43Pnzs2YyVPcBrk4FixYEN27d4/tttsu9txzz7jqqqvi8MMPL7jDzYoMHTo0485yW2+99SqPdcghh0TFihVj2LBhUaZMmfjxxx+LPHVvhx12iD59+hTa/tZbb41hw4alFko1atQokiSJhg0bFpqd9XsffvhhfPbZZzFkyJCMi5kWNTV7Re/d8m8l//j5L+qzv7J6X3rppWjdunWxmtndd989dt9997j88stj2LBhceyxx8aDDz6YcaFVgI2J/ieT/mfd9j/7779/ZGdnx3333bfCi53fe++9Ubp06TWaKVOzZs3Izc3NmFm+XFFja2r06NExe/bseOyxx2LvvfcuGJ86depaO8YfNWrUKN5///1o3779St/3p59+OhYvXhxPPfVUxoyv35/et1xxerDlp8hGrH4P9vPPPxdr5n7ZsmXj4IMPjoMPPjjy8/PjtNNOi1tvvTUuvvji1ZrhxsbFNaXY5Jx//vlRvnz5OOmkk2LmzJmFln/++ecFtyY94IADIiIK7hqx3PJvLg488MC1VlejRo1i3rx5Bd8aRfx6LYQ/3uHmj3dLi4iCb6D+eJvY5WrXrh0777xzDBkyJKPx++ijj+KFF14oeJ7rQrt27eLSSy+Nm2++uci7xy2XnZ1d6FvIRx55JL799tuMseXNY1EN7Oq64IIL4quvvoohQ4bEddddFw0aNIiuXbuu8HVcrnXr1tGhQ4eCn+KEUuXKlYvDDz88RowYEYMGDYry5cvHoYceWrD866+/jtdeey26dOkSRxxxRKGf7t27x5QpU+Ktt97608+7ODp37hzZ2dnRt2/fQu9LkiQxe/bsiPjtG8bfr5MkSZG3913Re1e/fv3Izs4uuObAcgMHDix2vV26dIm8vLy49NJLCy1btmxZwTF//PHHQs9nVb8/ABsD/c/cgnH9z7rvf+rWrRvdu3ePl156KQYNGlRo+eDBg2PUqFFx4oknFjolvziys7OjQ4cO8cQTT8R3331XMD5lypR47rnnVnt/KztORGafs2TJktXqUVZXly5d4ttvv43bb7+90LJFixbFggULVljbvHnz4u677y60Xfny5Yv87DRq1CgiIqMHW7BgwWrNXOrSpUuMHTs2Ro4cWWjZ3LlzY9myZRERBb3jcqVKlYodd9wxIvRgmzozpdjkNGrUKIYNGxZ/+9vfolmzZnH88cfHDjvsEEuWLIn//e9/8cgjj0S3bt0iImKnnXaKrl27xm233VYwffftt9+OIUOGxGGHHRbt2rVba3UdddRRccEFF8Thhx8eZ511VixcuDAGDRoU2267bcaFLvv16xevvfZaHHjggVG/fv34/vvvY+DAgbHVVlvFX/7ylxXu/+qrr479998/9thjjzjxxBMLbolcuXLlVZ569meUKlUq/v3vf69yvYMOOij69esX3bt3jz333DM+/PDDGDp0aKGGp1GjRlGlSpUYPHhwVKxYMcqXLx+tWrUq1vUHfm/UqFExcODA6NOnT8Etmu++++5o27ZtXHzxxXHVVVet1v6K47jjjot77703Ro4cGccee2zGt7PDhg2LJEnikEMOKXLbAw44IEqXLh1Dhw6NVq1aFYyPGzcu7r///kLrN2rUaJXfhK9Mo0aN4rLLLovevXvHtGnT4rDDDouKFSvG1KlT4/HHH49TTjklevXqFU2bNo1GjRpFr1694ttvv41KlSrF8OHDi7yOSIsWLSIi4qyzzopOnTpFdnZ2HHXUUVG5cuU48sgj46abboqsrKxo1KhRPPPMMxnXTFiVNm3aRI8ePaJ///4xYcKE6NixY5QpUyYmT54cjzzySNxwww1xxBFHxJAhQ2LgwIFx+OGHR6NGjeKnn36K22+/PSpVqrRO/+cEoKTpf/Q/Een2PwMGDIhPP/00TjvttHj++ecLZkSNHDkynnzyyWjTpk2RlysorksuuSReeOGFaN26dZx66qmRl5cXN998c+ywww4xYcKEtfIc9txzz6hatWp07do1zjrrrMjKyor77rtvtU7nXF1///vf4+GHH45//OMf8corr0Tr1q0jLy8vPv3003j44Ydj5MiR0bJly+jYsWPBzKMePXrEzz//HLfffnvUrFkzpk+fnrHPFi1axKBBg+Kyyy6Lxo0bR82aNWOfffaJjh07Rr169eLEE0+M8847L7Kzs+Ouu+6KGjVqxFdffVWses8777x46qmn4qCDDopu3bpFixYtYsGCBfHhhx/Go48+GtOmTYvq1avHSSedFHPmzIl99tknttpqq/jyyy/jpptuip133jljRiaboJTv9gfrjc8++yw5+eSTkwYNGiRly5ZNKlasmLRu3Tq56aabMm7vunTp0qRv375Jw4YNkzJlyiR169ZNevfunbFOkvx6m9QDDzyw0HH+eCveFd0SOUmS5IUXXkh22GGHpGzZskmTJk2S+++/v9CtWl9++eXk0EMPTerUqZOULVs2qVOnTnL00Udn3Ia1qFsiJ0mSvPTSS0nr1q2TcuXKJZUqVUoOPvjg5JNPPslYZ/nx/njL5aJuDVuU398SeUVWdEvkc889N6ldu3ZSrly5pHXr1snYsWOLvJXxk08+mWy33XYFt/xd/jzbtGmTbL/99kUe8/f7mT9/flK/fv1k1113TZYuXZqx3jnnnJOUKlUqGTt27Eqfw5pYtmxZUrt27SQikhEjRmQsa968eVKvXr2Vbt+2bdukZs2aydKlSwtewxX9/P62x0VZ0fv8R8OHD0/+8pe/JOXLl0/Kly+fNG3aNDn99NOTSZMmFazzySefJB06dEgqVKiQVK9ePTn55JOT999/v9BncNmyZcmZZ56Z1KhRI8nKysr4XP/www/JX//612SzzTZLqlatmvTo0SP56KOPCu1jVZ+v2267LWnRokVSrly5pGLFiknz5s2T888/P/nuu++SJPn19tNHH310Uq9evSQnJyepWbNmctBBByXvvvvuSl8HgI2F/kf/k2b/s3jx4mTAgAFJixYtkvLlyyebbbZZsuuuuybXX399smTJkkLrR0Ry+umnF7mviEj69OmTMfbyyy8nu+yyS1K2bNmkUaNGyR133JGce+65SW5ubsZ69evXz+iNlr+v77zzTsZ6r7zyShIRySuvvFIwNmbMmGT33XdPypUrl9SpUyc5//zzk5EjRxZar2vXrkn9+vVX+Zqs7P1absmSJcmVV16ZbL/99klOTk5StWrVpEWLFknfvn2TefPmFaz31FNPJTvuuGOSm5ubNGjQILnyyiuTu+66q9BndsaMGcmBBx6YVKxYMYmIjM/We++9l7Rq1SopW7ZsUq9eveS6664r8nO/ot/1JEmSn376Kendu3fSuHHjpGzZskn16tWTPffcM7nmmmsK3udHH3006dixY1KzZs2CY/Xo0SOZPn36Kl8zNm5ZSbIOY14AAABIyWGHHRYff/xxTJ48uaRLAYrBNaUAAADY4Pz+wusREZMnT44RI0ZE27ZtS6YgYLWZKQUAAMAGp3bt2tGtW7fYeuut48svv4xBgwbF4sWLY/z48bHNNtuUdHlAMbjQOQAAABuc/fbbLx544IGYMWNG5OTkxB577BFXXHGFQAo2IGZKAQAAAJA615QCAAAAIHVCKQAAAABSJ5QCAAAAIHUb5YXOy+1yRkmXAGwAvh97Y0mXAGwgKuZuGt/j6aGA4vjxnZtLugRgA5BbjMRp0+iwAAAAAFivCKUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1QCgAAAIDUCaUAAAAASJ1Qig1G610bxaPX94gvXrg8Fo2/OQ5uu+MK173xoqNi0fib44xj2maMf/ps31g0/uaMn17d9y1YXq/25oWWLxp/c/xf8wbr6FkBJe2eO2+Pljs1i2uvuqJg7JQTj4+WOzXL+Lni0ktKrkiANbSq/um2vscV6nuevPm0jHWqVtos7r68a8x8/eqY/tpVMajPMVG+XNmMdXbYpk68dOc/48c3B8Tk5y6Nnl07rPPnBqwfHhw2NPbfd5/YbZfmcexRR8aHH3xQ0iWxASld0gVAcZUvlxMffvZt3Pvk2HjoulNWuN4h7XaM/2veIL77fm6Ry/sOfCbufmxMweOfFiwutM7+PW6MiZ9PL3g8e96CNS8cWG99/NGH8dijD8U22zYptOzwvx4ZPU47s+Bxbm65NEsDWCuK0z+NHPNx9Ohzf8HjxUuWZSy/+4quUat65Tjo1JujTOnsuLXvcXHLxcdEt3/dExERFcvnxtMDz4hX3vo0zrz8wdhhmy1jcJ9jY+5Pi+Ku3/VcwMbn+edGxDVX9Y9/9+kbzZvvFEPvGxKn9jgxnnzm+ahWrVpJl8cGQCjFBuOFMZ/EC2M+Wek6dWpUjusuODIOPu2WePymU4tc5+cFv8TM2T+tdD9z5i5Y5TrAhm3hwgVxce/z4qI+/eLO2wcXWp6bmxvVq9cogcoA1p7i9E9LlixbYd/TpOEW0an19tH62Kti3CdfRUREzysfiSduOjV6D3g8pv8wL446oGWULZMdPS4ZGkuX5cXEL2bEjk22jLOOayeUgo3cfUPujs5HdInDDv9rRET8u0/feO210fHEY8PjxJNXPJEAlnP6HhuNrKysuPOy42PAkJdj4hczVrjeud07xjevXBljH7ggzjm+fWRnF/41ePT6HvHly/3j5bvOiQPbNF+XZQMl5MorLo3We7eJVrvvWeTy50Y8E+3b7BFdOh8cN99wXfyyaFHKFQKkY6+W28SXL/eP9x+/OG74199i88rlC5a12rFh/Dh/YUEgFREx6q1JkZ+fxG471C9YZ8y4KbF0WV7BOi/+b2I0aVgrqlQ0yxQ2VkuXLImJn3wcu+/xWy9VqlSp2H33PeOD98eXYGVsSEp0ptSsWbPirrvuirFjx8aMGb+GCLVq1Yo999wzunXrFjVq+Iaa4ju3+76xLC8/bnlg9ArXGfjAqzF+4tfx4/wFsftOW0e/Mw+JWjUqxwXXPhYREQsWLY4Lrn0sxk74PPLzkzisw87x8HUnR5eet8ezr36Y0jMB1rWRzz0bn078JO4d9kiRy/fb/6CoXbtO1KhZMyZ/Niluuv7a+HLa1Lh6wE0pVwpF00Oxtrz4v4nx5Kj3Y9q3s2PrrapH3zMPjidvPjXadL028vOT2KJapfhhTuYsqry8/Jgzf2FsUb1SRERsUa1STPt2dsY63///bbaoXinm/iTUh43Rj3N/jLy8vEKn6VWrVi2mTv2ihKpiQ1NiodQ777wTnTp1is022yw6dOgQ2267bUREzJw5M2688cb473//GyNHjoyWLVuudD+LFy+OxYszrwmU5OdFVqnsdVY7659dmtWN049uG3sec+VK17vx/lEF//3R5O9iydJlcfNFR8fFNz4VS5Yui9lzF2Ss894nX0XtGpXjnOPbC6VgIzFjxvS49qr+ccutd0ZOTk6R63Q+okvBfzfeZtuoXr1GnHpK9/jm669iq7r10ioViqSHYm16ZOR7Bf/98ZTv4sPJ38bEZ/rG3i23idFvf1aClQGwKSixUOrMM8+MI488MgYPHhxZWVkZy5IkiX/84x9x5plnxtixY1e6n/79+0ffvn0zxrK32C3K1P6/tV4z66/WuzSKmptXiM9G9CsYK106O/7bs3OccWy7aHpgnyK3e+fDaVGmTHbUr7N5TP7y+xWs82Xs06rpOqkbSN+nn3wcc+bMjuOO+mvBWF5eXox/7914+MFh8b933o/s7Mz/Kd+h+a93q/r6K6EUJU8Pxbo07dvZ8cOPP0WjujVi9NufxczZ86PG5hUz1snOLhWbV9osZs6aHxERM2fPjy2qZa5T8/9vs3wdYONTtUrVyM7OjtmzM2dKzp49O6pXr15CVbGhKbFQ6v3334977rmnUDMV8eu1gc4555zYZZddVrmf3r17R8+ePTPGau51wVqrkw3DsGffiVFvTcoYe3rg6THs2bfj3iffXOF2OzXZKvLy8gtNS/+9HZtsGTM0VLDR2K3VHvHgo09mjPXrc1HUb9AwunY/qVAgFRExadKnERFR3SlRrAf0UKxLW9asEtUqly/ofd76YGpUrbRZ7NKsboyf+HVERLTdbdsoVSor3vnoy4J1Ljn94ChdulQsW5YfERHtd28ak6bOcOoebMTKlC0bzbbbPt56c2zs075DRETk5+fHW2+NjaOOPq6Eq2NDUWKhVK1ateLtt9+Opk2LnoHy9ttvxxZbbLHK/eTk5BQ6/cK0841T+XJlo1Hd3/6HsMGW1WLHbbeMH+cvjK9n/Bhz5i3IWH/psryYOWt+wQyoVjs2jN12qB+vvjs5flrwS+y+Y8O4stdf44ER7xQ0TMce3CqWLl0WEz79JiIiDt1np+h66B5xar9hKT1LYF0rX758NN5m24yx3HLlokqVKtF4m23jm6+/iudHPBOt92oTlStXicmTJ8V1V/83dm3RMrbZtkkJVQ2/0UOxOlbWP82ZtyAu6nFAPPHyhJgxa35sXbd6XH72YfH517Pixf9NjIiISVNnxsgxH8ctFx8TZ13+YJQpnR0DLuwSj4wcF9N/mBcREQ89927865QDYnCfY+Pau1+M7RvXidOPaRvnX/NYiTxnID1/79o9Lv7XBbH99jvEDs13jPvvGxKLFi2Kww7vXNKlsYEosVCqV69eccopp8R7770X7du3L2ieZs6cGS+//HLcfvvtcc0115RUeayHdt2ufrxwx9kFj6/q9eupN/c99Wac0uf+VW6/eMnSOLJTi7joHwdETpnSMe272XHT0FfixvtGZax34cn7Rb3am8eyZfnx2bSZ8fcL74rHX5qwVp8LsP4qXaZMvP3W2Hhg6L2xaNGi2KJWrdinw75x4smnlnRpEBF6KFbPyvqns654KHbYZss49uBWUaViuZj+w7x4aeyn0W/gM7Fk6bKCbbr/a0gMuLBLjLj1zMjPT+KJlyfEuVf9dqOI+T//EgefdnNcf2GX+N+wC2L23J+j/23PxV2PjUnviQIlYr/9D4gf58yJgTffGLNm/RBNmjaLgbfeEdWcvkcxZSVJkpTUwR966KEYMGBAvPfee5GX9+stZLOzs6NFixbRs2fP6NKlyyr2ULRyu5yxNssENlLfj72xpEsANhAVc0uVdAkZ9FBASfrxnZtLugRgA5BbjGlQJRpKLbd06dKYNWtWRERUr149ypQp86f2p6ECikMoBRTX+hZKLaeHAkqCUAoojuKEUiV2+t7vlSlTJmrXrl3SZQAAbFD0UADAhmz9/NoPAAAAgI2aUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEjdGoVSr7/+ehx33HGxxx57xLfffhsREffdd1+88cYba7U4AICNhf4JACDTaodSw4cPj06dOkW5cuVi/PjxsXjx4oiImDdvXlxxxRVrvUAAgA2d/gkAoLDVDqUuu+yyGDx4cNx+++1RpkyZgvHWrVvHuHHj1mpxAAAbA/0TAEBhqx1KTZo0Kfbee+9C45UrV465c+eujZoAADYq+icAgMJWO5SqVatWTJkypdD4G2+8EVtvvfVaKQoAYGOifwIAKGy1Q6mTTz45zj777HjrrbciKysrvvvuuxg6dGj06tUrTj311HVRIwDABk3/BABQWOnV3eDCCy+M/Pz8aN++fSxcuDD23nvvyMnJiV69esWZZ565LmoEANig6Z8AAArLSpIkWZMNlyxZElOmTImff/45tttuu6hQocLarm2NldvljJIuAdgAfD/2xpIuAdhAVMxd7cnlRVqf+6cIPRRQPD++c3NJlwBsAHKLMQ1qtWdKLVe2bNnYbrvt1nRzAIBNjv4JAOA3qx1KtWvXLrKysla4fNSoUX+qIACAjY3+CQCgsNUOpXbeeeeMx0uXLo0JEybERx99FF27dl1bdQEAbDT0TwAAha12KDVgwIAixy+55JL4+eef/3RBAAAbG/0TAEBha+eqnRFx3HHHxV133bW2dgcAsNHTPwEAm7K1FkqNHTs2cnNz19buAAA2evonAGBTttqn73Xu3DnjcZIkMX369Hj33Xfj4osvXmuF/Rnfv+k278Cqlclea7k8wEptCP1TRMTMsXooACA9qx1KVa5cOeNxqVKlokmTJtGvX7/o2LHjWisMAGBjoX8CACgsK0mSpLgr5+XlxZgxY6J58+ZRtWrVdVnXn/LT4vySLgHYAJgpBRRX7mp/jfebDaV/ioiY/4seCli1sqX1UMCqFad/Wq2/JtnZ2dGxY8eYO3fuGpYEALBp0T8BABRttSPuHXbYIb744ot1UQsAwEZJ/wQAUNhqh1KXXXZZ9OrVK5555pmYPn16zJ8/P+MHAIBM+icAgMKKfU2pfv36xbnnnhsVK1b8beOsrIL/TpIksrKyIi8vb+1XuZpcUwooDteUAoprTa8ptSH1TxGuKQUUj2tKAcVRnP6p2KFUdnZ2TJ8+PSZOnLjS9dq0aVOs4tYloRRQHEIpoLjWNJTakPqnCKEUUDxCKaA41mooVapUqZgxY0bUrFnzz9a1zgmlgOIQSgHFtaah1IbUP0UIpYDiEUoBxbHW7773++nmAACsmv4JAKBoqzVTqnLlyqtsrObMmbNWCvszzJQCisNMKaC4/sxMqQ2lf4owUwooHjOlgOIoTv+0Wi1W3759o3LlymtaDwDAJkf/BABQNNeUAjZZZkoBxeWaUgC/MVMKKI61ek0p10MAAFg9+icAgBUrdihVzAlVAAD8f/onAIAVK/Zk9Px807kBAFaH/gkAYMWcDAwAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSAAAAAKROKAUAAABA6oRSbLTuufP2aLljs7j2yisKxi7v1ycOPaBjtN5t5+jQZs/oedbpMW3qFyVYJbA+GHTLTbHT9k0yfg49aL+SLgsgdffceXvstlOzuPaq3/qnWbN+iP/86/zotM9esVerXeO4v3WOUS+9UIJVAuuTB4cNjf333Sd226V5HHvUkfHhBx+UdElsQEqXdAGwLnz80Yfx2CMPxTbbNskYb7bd9rH/AQdFrdp1Yv68uXHroFvi9B4nxVPPvRjZ2dklVC2wPmjUeJu47Y67Cx5nl/Y3Adi0fPzRh/H4o4X7p0suujB++umnuO6GW6Jy1aoxcsQz0fu8c+LeYY9Ek2bblVC1wPrg+edGxDVX9Y9/9+kbzZvvFEPvGxKn9jgxnnzm+ahWrVpJl8cGwEwpNjoLFy6Ii3ufFxdd0i8qVqqUsazzEV1i15a7RZ0tt4ym220fp515dsycMT2mf/dtCVULrC9KZ2dH9Ro1Cn6qVt28pEsCSM3ChQviP73Pi3/1Kdw/ffD+hPjb0cfG9s13jK22qhsnnnJqVKxYMSZO/LiEqgXWF/cNuTs6H9ElDjv8r9GoceP4d5++kZubG088NrykS2MDIZRio3Pl5ZdG673aRKvd91zpeosWLoynnngsttxyq9iiVq2UqgPWV19+9WV0aPuXOKBT++h9/rkx/bvvSrokgNRcdcWl0XrvovunHXfaOV4c+VzMmzc38vPz44Xnno3Fi5dEi5b/VwKVAuuLpUuWxMRPPo7d9/jt70apUqVi9933jA/eH1+ClbEhWa9Dqa+//jpOOOGEki6DDcjI556NTyd+Emec3XOF6zzy4LDYq1WL2Gv3FvG/N16PW267M8qUKZtilcD6pvmOO8all/ePgbfeERddfEl8++230f34Y2PBgp9LujRYI3ooVscL/79/Ov2sovun/lcPiGXLlkWHvfeIPXfbKa647JK4esBNUbde/ZQrBdYnP879MfLy8gqdpletWrWYNWtWCVXFhma9DqXmzJkTQ4YMWek6ixcvjvnz52f8LF68OKUKWZ/MmDE9rr2yf1z236sjJydnhevtf+DBMfTh4XHbXfdGvfoN4sJe5/jMwCbuL3u1iY6d9o9tmzSN1n/ZK24edFv89NP8GPn8cyVdGqwRPRTFNWPG9Lj2qv5xaf8V90+Db7kxfvrpp7jltrvi3mGPxLF/7xa9zz8npkz+LOVqAdjYlOiFzp966qmVLv/ii1XfFa1///7Rt2/fjLELL/pP/OviPn+qNjY8n37yccyZMzuO+9tfC8by8vJi/HvvxsMPDov/vft+ZGdnR4WKFaNCxYpRr36DaL7TTtGu9e7xyssvxX4HHFiC1QPrk0qVKkX9+g3i66++KulSoEjrsofq/W891KZkef/096MK90+PPDgsHn1yRDz84NB4cPhT0ajxNhERsW2TpjF+3K/Le198SQlVDpS0qlWqRnZ2dsyePTtjfPbs2VG9evUSqooNTYmGUocddlhkZWVFkiQrXCcrK2ul++jdu3f07Jk51XhJlFkr9bFh2a3VHvHg8Cczxvr956Ko37BhdO1+UpF310uSiCSSWLp0SVplAhuAhQsWxNdffx0HHlKjpEuBIq2rHmpxoofa1OzWao944NE/9E99LooGDRrG8d1Pil9++SUifr1OzO9ll8qO/CQ/tTqB9U+ZsmWj2Xbbx1tvjo192neIiIj8/Px4662xcdTRx5VwdWwoSjSUql27dgwcODAOPfTQIpdPmDAhWrRosdJ95OTkFJpq/NNi/0BuisqXLx+Nt9k2Yyy3XLmoUrlKNN5m2/jmm6/jxeefi933bB1Vq1aNmTNnxj133h65OTnR+i97l1DVwPrg2quvjDZt20XtOnXih++/j0G33BTZ2aVi/wMOKunSoEjrqoea/4sealNTVP9Urly5qFzl1/5p2dKlUbdeveh/aZ84u+f5UblKlRg96uV4683/xYCbBpVQ1cD64u9du8fF/7ogtt9+h9ih+Y5x/31DYtGiRXHY4Z1LujQ2ECUaSrVo0SLee++9FTZUq/oGEFZHTtmcGD/u3Xjg/ntj/vz5Ua1atdilRcu4894HYvM/XJwP2LTMnDkjLjyvZ8ydOzeqbr557LJri7hv2MOx+eabl3RpUCQ9FGkpXaZMXH/zrXHzDddFz7NOi4ULF0bdevXikkv7R+u92pR0eUAJ22//A+LHOXNi4M03xqxZP0STps1i4K13RDWn71FMWUkJdiyvv/56LFiwIPbbb78ily9YsCDefffdaNNm9f7BM1MKKI4y2ev1vR6A9UhuiX6NV9i66qHMlAKKo2xpPRSwasXpn0o0lFpXhFJAcQilgOJa30KpdUUoBRSHUAoojuL0T/6aAAAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJC6rCRJkpIuAta1xYsXR//+/aN3796Rk5NT0uUA6yl/KwB+428iUBz+VvBnCKXYJMyfPz8qV64c8+bNi0qVKpV0OcB6yt8KgN/4mwgUh78V/BlO3wMAAAAgdUIpAAAAAFInlAIAAAAgdUIpNgk5OTnRp08fF94DVsrfCoDf+JsIFIe/FfwZLnQOAAAAQOrMlAIAAAAgdUIpAAAAAFInlAIAAAAgdUIpNnq33HJLNGjQIHJzc6NVq1bx9ttvl3RJwHrmtddei4MPPjjq1KkTWVlZ8cQTT5R0SQAlTg8FrIoeij9LKMVG7aGHHoqePXtGnz59Yty4cbHTTjtFp06d4vvvvy/p0oD1yIIFC2KnnXaKW265paRLAVgv6KGA4tBD8We5+x4btVatWsVuu+0WN998c0RE5OfnR926dePMM8+MCy+8sISrA9ZHWVlZ8fjjj8dhhx1W0qUAlBg9FLC69FCsCTOl2GgtWbIk3nvvvejQoUPBWKlSpaJDhw4xduzYEqwMAGD9pYcCIC1CKTZas2bNiry8vNhiiy0yxrfYYouYMWNGCVUFALB+00MBkBahFAAAAACpE0qx0apevXpkZ2fHzJkzM8ZnzpwZtWrVKqGqAADWb3ooANIilGKjVbZs2WjRokW8/PLLBWP5+fnx8ssvxx577FGClQEArL/0UACkpXRJFwDrUs+ePaNr167RsmXL+L//+7+4/vrrY8GCBdG9e/eSLg1Yj/z8888xZcqUgsdTp06NCRMmxOabbx716tUrwcoASoYeCigOPRR/VlaSJElJFwHr0s033xxXX311zJgxI3beeee48cYbo1WrViVdFrAeGT16dLRr167QeNeuXeOee+5JvyCA9YAeClgVPRR/llAKAAAAgNS5phQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRQAAAAAqRNKAQAAAJA6oRSwSejWrVscdthhBY/btm0b//znP1OvY/To0ZGVlRVz585N/dgAAKtLDwWsS0IpoER169YtsrKyIisrK8qWLRuNGzeOfv36xbJly9bpcR977LG49NJLi7WuJggAWN/ooYCNQemSLgBgv/32i7vvvjsWL14cI0aMiNNPPz3KlCkTvXv3zlhvyZIlUbZs2bVyzM0333yt7AcAoKTooYANnZlSQInLycmJWrVqRf369ePUU0+NDh06xFNPPVUwXfzyyy+POnXqRJMmTSIi4uuvv44uXbpElSpVYvPNN49DDz00pk2bVrC/vLy86NmzZ1SpUiWqVasW559/fiRJknHMP049X7x4cVxwwQVRt27dyMnJicaNG8edd94Z06ZNi3bt2kVERNWqVSMrKyu6desWERH5+fnRv3//aNiwYZQrVy522mmnePTRRzOOM2LEiNh2222jXLly0a5du4w6AQD+DD0UsKETSgHrnXLlysWSJUsiIuLll1+OSZMmxYsvvhjPPPNMLF26NDp16hQVK1aM119/PcaMGRMVKlSI/fbbr2Cba6+9Nu65556466674o033og5c+bE448/vtJjHn/88fHAAw/EjTfeGBMnToxbb701KlSoEHXr1o3hw4dHRMSkSZNi+vTpccMNN0RERP/+/ePee++NwYMHx8cffxznnHNOHHfccfHqq69GxK+NX+fOnePggw+OCRMmxEknnRQXXnjhunrZAIBNnB4K2NA4fQ9YbyRJEi+//HKMHDkyzjzzzPjhhx+ifPnycccddxRMOb///vsjPz8/7rjjjsjKyoqIiLvvvjuqVKkSo0ePjo4dO8b1118fvXv3js6dO0dExODBg2PkyJErPO5nn30WDz/8cLz44ovRoUOHiIjYeuutC5Yvn6Zes2bNqFKlSkT8+q3gFVdcES+99FLsscceBdu88cYbceutt0abNm1i0KBB0ahRo7j22msjIqJJkybx4YcfxpVXXrkWXzUAYFOnhwI2VEIpoMQ988wzUaFChVi6dGnk5+fHMcccE5dcckmcfvrp0bx584xrILz//vsxZcqUqFixYsY+fvnll/j8889j3rx5MX369GjVqlXBstKlS0fLli0LTT9fbsKECZGdnR1t2rQpds1TpkyJhQsXxr777psxvmTJkthll10iImLixIkZdUREQfMFAPBn6aGADZ1QCihx7dq1i0GDBkXZsmWjTp06Ubr0b3+aypcvn7Huzz//HC1atIihQ4cW2k+NGjXW6PjlypVb7W1+/vnniIh49tlnY8stt8xYlpOTs0Z1AACsDj0UsKETSgElrnz58tG4ceNirbvrrrvGQw89FDVr1oxKlSoVuU7t2rXjrbfeir333jsiIpYtWxbvvfde7LrrrkWu37x588jPz49XX321YOr57y3/ljEvL69gbLvttoucnJz46quvVvjtYLNmzeKpp57KGHvzzTdX/SQBAIpBDwVs6FzoHNigHHvssVG9evU49NBD4/XXX4+pU6fG6NGj46yzzopvvvkmIiLOPvvs+O9//xtPPPFEfPrpp3HaaafF3LlzV7jPBg0aRNeuXeOEE06IJ554omCfDz/8cERE1K9fP7KysuKZZ56JH374IX7++eeoWLFi9OrVK84555wYMmRIfP755zFu3Li46aabYsiQIRER8Y9//CMmT54c5513XkyaNCmGDRsW99xzz7p+iQAACtFDAesjoRSwQdlss83itddei3r16kXnzp2jWbNmceKJJ8Yvv/xS8K3fueeeG3//+9+ja9eusccee0TFihXj8MMPX+l+Bw0aFEcccUScdtpp0bRp0zj55JNjwYIFERGx5ZZbRt++fePCCy+MLbbYIs4444yIiLj00kvj4osvjv79+0ezZs1iv/32i2effTYaNmwYERH16tWL4cOHxxNPPBE77bRTDB48OK644op1+OoAABRNDwWsj7KSFV21DgAAAADWETOlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1AmlAAAAAEidUAoAAACA1P0/RODhXhT4sl8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLP Classifier Comparison: VAE Features vs Original Features\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "# Directory with the VAE dataset\n",
    "data_dir = 'vae_dataset'\n",
    "\n",
    "# Load train data\n",
    "print(\"Loading train dataset...\")\n",
    "X_train_vae = np.load(os.path.join(data_dir, 'train_combined_features.npy'))\n",
    "feature_names = np.load(os.path.join(data_dir, 'train_feature_names.npy'), allow_pickle=True)\n",
    "y_train = np.load(os.path.join(data_dir, 'train_labels.npy'))\n",
    "X_train_original = np.load(os.path.join(data_dir, 'train_original_data.npy'))\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test dataset...\")\n",
    "X_test_vae = np.load(os.path.join(data_dir, 'test_combined_features.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, 'test_labels.npy'))\n",
    "X_test_original = np.load(os.path.join(data_dir, 'test_original_data.npy'))\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Train VAE features shape: {X_train_vae.shape}\")\n",
    "print(f\"Train original features shape: {X_train_original.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test VAE features shape: {X_test_vae.shape}\")\n",
    "print(f\"Test original features shape: {X_test_original.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Create and train MLP on VAE features\n",
    "print(\"\\nTraining MLP on VAE features...\")\n",
    "start_time = time()\n",
    "mlp_vae = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "mlp_vae.fit(X_train_vae, y_train)\n",
    "vae_training_time = time() - start_time\n",
    "print(f\"Training completed in {vae_training_time:.2f} seconds\")\n",
    "\n",
    "# Create and train MLP on original features\n",
    "print(\"\\nTraining MLP on original features...\")\n",
    "start_time = time()\n",
    "mlp_original = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "mlp_original.fit(X_train_original, y_train)\n",
    "original_training_time = time() - start_time\n",
    "print(f\"Training completed in {original_training_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate on test set - VAE features\n",
    "print(\"\\nEvaluating MLP with VAE features on test set...\")\n",
    "y_pred_vae = mlp_vae.predict(X_test_vae)\n",
    "y_pred_proba_vae = mlp_vae.predict_proba(X_test_vae)[:, 1]\n",
    "\n",
    "accuracy_vae = accuracy_score(y_test, y_pred_vae)\n",
    "roc_auc_vae = roc_auc_score(y_test, y_pred_proba_vae)\n",
    "report_vae = classification_report(y_test, y_pred_vae)\n",
    "\n",
    "print(f\"Accuracy (VAE features): {accuracy_vae:.4f}\")\n",
    "print(f\"ROC AUC (VAE features): {roc_auc_vae:.4f}\")\n",
    "print(\"Classification Report (VAE features):\")\n",
    "print(report_vae)\n",
    "\n",
    "# Evaluate on test set - Original features\n",
    "print(\"\\nEvaluating MLP with original features on test set...\")\n",
    "y_pred_original = mlp_original.predict(X_test_original)\n",
    "y_pred_proba_original = mlp_original.predict_proba(X_test_original)[:, 1]\n",
    "\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "roc_auc_original = roc_auc_score(y_test, y_pred_proba_original)\n",
    "report_original = classification_report(y_test, y_pred_original)\n",
    "\n",
    "print(f\"Accuracy (original features): {accuracy_original:.4f}\")\n",
    "print(f\"ROC AUC (original features): {roc_auc_original:.4f}\")\n",
    "print(\"Classification Report (original features):\")\n",
    "print(report_original)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'ROC AUC', 'Training Time (s)'],\n",
    "    'VAE Features': [accuracy_vae, roc_auc_vae, vae_training_time],\n",
    "    'Original Features': [accuracy_original, roc_auc_original, original_training_time]\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Create confusion matrices\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cm_vae = confusion_matrix(y_test, y_pred_vae)\n",
    "sns.heatmap(cm_vae, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix - VAE Features')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cm_original = confusion_matrix(y_test, y_pred_original)\n",
    "sns.heatmap(cm_original, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix - Original Features')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit_vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
