{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_path=\"data/creditcard.csv\"\n",
    "df = pd.read_csv(df_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "if 'Class' in df.columns:\n",
    "    class_counts = df['Class'].value_counts()\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(class_counts)\n",
    "    print(f\"\\nPercentage of fraud cases: {class_counts[1] / len(df) * 100:.4f}%\")\n",
    "    print(f\"Imbalance ratio: 1:{class_counts[0] / class_counts[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import HfApi, login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#set seed\n",
    "np.random.seed(42)\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get HF API token from environment variables\n",
    "hf_token = os.getenv(\"HF_API_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"Warning: HF_API_TOKEN not found in .env file\")\n",
    "else:\n",
    "    print(\"Successfully loaded HF_API_TOKEN from .env file\")\n",
    "\n",
    "# 1. Normalize Time and Amount columns\n",
    "print(\"Normalizing Time and Amount columns...\")\n",
    "scaler_time = StandardScaler()\n",
    "scaler_amount = StandardScaler()\n",
    "\n",
    "# Fit the scalers on the entire dataset\n",
    "df['Time_norm'] = scaler_time.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "df['Amount_norm'] = scaler_amount.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Store the normalization statistics for future reference\n",
    "time_stats = {\n",
    "    'mean': float(scaler_time.mean_[0]),\n",
    "    'std': float(scaler_time.scale_[0])\n",
    "}\n",
    "amount_stats = {\n",
    "    'mean': float(scaler_amount.mean_[0]),\n",
    "    'std': float(scaler_amount.scale_[0])\n",
    "}\n",
    "normalization_stats = {\n",
    "    'Time': time_stats,\n",
    "    'Amount': amount_stats\n",
    "}\n",
    "\n",
    "print(\"Normalization statistics:\")\n",
    "print(f\"Time: mean={time_stats['mean']:.2f}, std={time_stats['std']:.2f}\")\n",
    "print(f\"Amount: mean={amount_stats['mean']:.2f}, std={amount_stats['std']:.2f}\")\n",
    "\n",
    "# 2. Drop the original Time and Amount columns and rename the normalized ones\n",
    "df = df.drop(['Time', 'Amount'], axis=1)\n",
    "df = df.rename(columns={'Time_norm': 'Time', 'Amount_norm': 'Amount'})\n",
    "\n",
    "# 3. Add an index column to help track original indices\n",
    "df['original_index'] = np.arange(len(df))\n",
    "\n",
    "# 4. Create stratified train/validation/test splits (80/10/10)\n",
    "# First split: 80% train, 20% temp\n",
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "print(\"\\nCreating stratified splits (80% train, 10% validation, 10% test)...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: split the temp data into test and validation (50% each, resulting in 10% of original data each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# 5. Create dataframes for each split\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# 6. Check the distribution in each split\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "print(f\"Train set - Total: {len(y_train)}, Fraud: {sum(y_train)}, Percentage: {sum(y_train)/len(y_train)*100:.4f}%\")\n",
    "print(f\"Validation set - Total: {len(y_val)}, Fraud: {sum(y_val)}, Percentage: {sum(y_val)/len(y_val)*100:.4f}%\")\n",
    "print(f\"Test set - Total: {len(y_test)}, Fraud: {sum(y_test)}, Percentage: {sum(y_test)/len(y_test)*100:.4f}%\")\n",
    "\n",
    "# 7. Convert to Hugging Face Dataset format\n",
    "print(\"\\nConverting to Hugging Face Dataset format...\")\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# 8. Output some information about the datasets\n",
    "print(\"\\nDataset split information:\")\n",
    "print(f\"Train: {train_dataset.shape}\")\n",
    "print(f\"Validation: {val_dataset.shape}\")\n",
    "print(f\"Test: {test_dataset.shape}\")\n",
    "\n",
    "# 9. Preview columns and first few examples\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(train_dataset.column_names)\n",
    "print(\"\\nFirst example from train set:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "# 10. Save the datasets locally (optional but useful for verification)\n",
    "print(\"\\nSaving datasets locally...\")\n",
    "dataset_dict.save_to_disk(\"credit_card_fraud_dataset\")\n",
    "\n",
    "# 11. To upload to HuggingFace\n",
    "def upload_to_huggingface(dataset_dict, repo_name, token):\n",
    "    # Login to HuggingFace\n",
    "    login(token)\n",
    "    \n",
    "    # Get HF API\n",
    "    api = HfApi()\n",
    "    \n",
    "    # Create repository if it doesn't exist\n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_name, exist_ok=True)\n",
    "        print(f\"Repository {repo_name} ready.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Push dataset to HuggingFace\n",
    "    dataset_dict.push_to_hub(repo_name)\n",
    "    \n",
    "    # Also save normalization stats as README.md\n",
    "    readme_content = f\"\"\"# Credit Card Fraud Dataset\n",
    "\n",
    "This dataset contains normalized credit card transaction data for fraud detection.\n",
    "\n",
    "## Normalization Statistics\n",
    "- Time: mean={time_stats['mean']:.4f}, std={time_stats['std']:.4f}\n",
    "- Amount: mean={amount_stats['mean']:.4f}, std={amount_stats['std']:.4f}\n",
    "\n",
    "## Class Distribution\n",
    "- Train set: {sum(y_train)} fraud out of {len(y_train)} ({sum(y_train)/len(y_train)*100:.4f}%)\n",
    "- Validation set: {sum(y_val)} fraud out of {len(y_val)} ({sum(y_val)/len(y_val)*100:.4f}%)\n",
    "- Test set: {sum(y_test)} fraud out of {len(y_test)} ({sum(y_test)/len(y_test)*100:.4f}%)\n",
    "\n",
    "## Features\n",
    "- Original Time and Amount columns have been normalized\n",
    "- 'original_index' column refers to the index in the original dataset\n",
    "\"\"\"\n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"README.md\",\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_name,\n",
    "        commit_message=\"Add README with normalization stats\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset successfully uploaded to https://huggingface.co/datasets/{repo_name}\")\n",
    "\n",
    "# Upload to Hugging Face with your username\n",
    "if hf_token:\n",
    "    username = \"stanpony\"\n",
    "    repo_name = f\"{username}/full_european_credit_card_fraud_dataset\"\n",
    "    upload_to_huggingface(dataset_dict, repo_name, hf_token)\n",
    "else:\n",
    "    print(\"Skipping upload to Hugging Face as no API token was found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset without try/except\n",
    "dataset = load_dataset(\"stanpony/full_european_credit_card_fraud_dataset\")\n",
    "print(\"Successfully loaded the dataset!\")\n",
    "\n",
    "# Print available splits\n",
    "print(\"\\nAvailable splits:\", list(dataset.keys()))\n",
    "\n",
    "# Print statistics for each split\n",
    "for split_name, split_dataset in dataset.items():\n",
    "    print(f\"\\n{split_name.upper()} split:\")\n",
    "    print(f\"- Number of samples: {len(split_dataset)}\")\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    split_df = split_dataset.to_pandas()\n",
    "    \n",
    "    # Check class distribution\n",
    "    if 'Class' in split_df.columns:\n",
    "        class_counts = split_df['Class'].value_counts()\n",
    "        print(f\"- Class distribution: {dict(class_counts)}\")\n",
    "        fraud_percentage = class_counts.get(1, 0) / len(split_df) * 100\n",
    "        print(f\"- Fraud percentage: {fraud_percentage:.4f}%\")\n",
    "    \n",
    "    # Check if Time and Amount are normalized\n",
    "    if 'Time' in split_df.columns and 'Amount' in split_df.columns:\n",
    "        print(f\"- Time column stats: mean={split_df['Time'].mean():.4f}, std={split_df['Time'].std():.4f}\")\n",
    "        print(f\"- Amount column stats: mean={split_df['Amount'].mean():.4f}, std={split_df['Amount'].std():.4f}\")\n",
    "    \n",
    "    # Check if original_index column exists\n",
    "    if 'original_index' in split_df.columns:\n",
    "        print(\"- original_index column exists ✓\")\n",
    "    else:\n",
    "        print(\"- original_index column missing ✗\")\n",
    "        \n",
    "    # Check for any missing values\n",
    "    missing_values = split_df.isnull().sum().sum()\n",
    "    print(f\"- Missing values: {missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkstern/miniconda3/envs/credit_vae/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the dataset!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"stanpony/full_european_credit_card_fraud_dataset\")\n",
    "print(\"Successfully loaded the dataset!\")\n",
    "\n",
    "# Convert each split to a pandas DataFrame\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_validation = dataset['validation'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()\n",
    "\n",
    "# Concatenate all splits into a single DataFrame\n",
    "df_all = pd.concat([df_train, df_validation, df_test], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print basic info about the combined DataFrame\n",
    "print(\"Combined DataFrame shape:\", df_all.shape)\n",
    "display(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df_all is your combined DataFrame containing a 'Class' column \n",
    "# where 0 = normal and 1 = anomalous\n",
    "# Remove columns 'original_index' and '__index_level_0__' if they exist\n",
    "columns_to_remove = ['original_index', '__index_level_0__']\n",
    "df_all = df_all.drop(columns=columns_to_remove, errors='ignore')\n",
    "# Separate normal and anomalous data\n",
    "normal = df_all[df_all['Class'] == 0]\n",
    "anomalous = df_all[df_all['Class'] == 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, anderson_ksamp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Define the feature columns: all columns that start with 'V' plus 'Time' and 'Amount'\n",
    "feature_columns = [col for col in normal.columns if col.startswith('V')] + ['Time', 'Amount']\n",
    "\n",
    "print(\"Performing KS and Anderson-Darling tests on each feature:\\n\")\n",
    "\n",
    "results = []  # to store results for later comparison\n",
    "\n",
    "for feature in feature_columns:\n",
    "    # Perform the two-sample KS test (univariate, handles unequal sample sizes)\n",
    "    ks_stat, ks_p = ks_2samp(normal[feature], anomalous[feature])\n",
    "    \n",
    "    # Perform the Anderson-Darling k-sample test.\n",
    "    # This test also works with unequal sample sizes and gives extra weight to the tails.\n",
    "    ad_result = anderson_ksamp([normal[feature].values, anomalous[feature].values])\n",
    "    \n",
    "    results.append((feature, ks_stat, ks_p, ad_result.statistic, ad_result.significance_level))\n",
    "    \n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"  KS test:  Statistic = {ks_stat:.4f}, p-value = {ks_p:.4g}\")\n",
    "    print(f\"  AD test:  Statistic = {ad_result.statistic:.4f}, significance level = {ad_result.significance_level:.4f}\\n\")\n",
    "\n",
    "# Optionally, you could convert results to a DataFrame for further inspection:\n",
    "results_df = pd.DataFrame(results, columns=[\"Feature\", \"KS_stat\", \"KS_p\", \"AD_stat\", \"AD_sig\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, anderson_ksamp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Assuming `normal` and `anomalous` DataFrames are already defined ---\n",
    "\n",
    "# Define the feature columns: all columns that start with 'V' plus 'Time' and 'Amount'\n",
    "feature_columns = [col for col in normal.columns if col.startswith('V')] + ['Time', 'Amount']\n",
    "\n",
    "# Perform KS and AD tests\n",
    "results = []\n",
    "for feature in feature_columns:\n",
    "    ks_stat, ks_p = ks_2samp(normal[feature], anomalous[feature])\n",
    "    ad_result = anderson_ksamp([normal[feature].values, anomalous[feature].values])\n",
    "    results.append((feature, ks_stat, ks_p, ad_result.statistic, ad_result.significance_level))\n",
    "\n",
    "# Create a DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=[\"Feature\", \"KS_stat\", \"KS_p\", \"AD_stat\", \"AD_sig\"])\n",
    "\n",
    "# --- Visualization: Dual y-axis Grouped Bar Chart ---\n",
    "x = np.arange(len(results_df))  # positions for each feature\n",
    "width = 0.35  # width of each bar\n",
    "\n",
    "# Make the figure taller so labels fit\n",
    "fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot KS statistics on the left y-axis\n",
    "bars1 = ax1.bar(x - width/2, results_df[\"KS_stat\"], width,\n",
    "                label=\"KS Statistic\", color=\"skyblue\")\n",
    "ax1.set_ylabel(\"KS Statistic (0–1 scale)\")\n",
    "\n",
    "# Dynamically set y-limit for KS\n",
    "max_ks = results_df[\"KS_stat\"].max()\n",
    "ax1.set_ylim(0, max_ks + 0.2)  # Add some buffer above max\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_df[\"Feature\"], rotation=90)\n",
    "\n",
    "# Create a twin y-axis for AD statistics\n",
    "ax2 = ax1.twinx()\n",
    "bars2 = ax2.bar(x + width/2, results_df[\"AD_stat\"], width,\n",
    "                label=\"AD Statistic\", color=\"salmon\")\n",
    "ax2.set_ylabel(\"AD Statistic (integrated difference)\")\n",
    "\n",
    "# Dynamically set y-limit for AD\n",
    "max_ad = results_df[\"AD_stat\"].max()\n",
    "ax2.set_ylim(0, max_ad * 1.2)\n",
    "\n",
    "# Annotate KS bars with vertical **bold** p-values\n",
    "for bar, p_val in zip(bars1, results_df[\"KS_p\"]):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height + 0.02,\n",
    "        r\"$\\mathbf{p=%.4f}$\" % p_val,\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=8,\n",
    "        color='blue',\n",
    "        rotation=90\n",
    "    )\n",
    "\n",
    "# Annotate AD bars with vertical **bold** significance levels\n",
    "for bar, sig in zip(bars2, results_df[\"AD_sig\"]):\n",
    "    height = bar.get_height()\n",
    "    offset = max_ad * 0.02\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height + offset,\n",
    "        r\"$\\mathbf{sig=%.4f}$\" % sig,\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=8,\n",
    "        color='red',\n",
    "        rotation=90\n",
    "    )\n",
    "\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\")\n",
    "\n",
    "plt.title(\"Comparison of KS and Anderson–Darling Test Statistics by Feature\")\n",
    "\n",
    "# Auto-adjust layout, then add extra space on top\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.90)  # If still clipped, try 0.85 or 0.8\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recon error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: Set global font sizes (can be overridden locally)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"font.size\": 11  # general default font size\n",
    "})\n",
    "\n",
    "# Create a 4x2 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(14, 18))\n",
    "\n",
    "# Loop through each key in the specified order\n",
    "for idx, key in enumerate(order):\n",
    "    row_idx = idx // 2\n",
    "    col_idx = idx % 2\n",
    "    ax = axes[row_idx, col_idx]\n",
    "    \n",
    "    stats_normal = data[key][\"normal\"]\n",
    "    stats_fraud  = data[key][\"fraud\"]\n",
    "    box_data = [stats_normal, stats_fraud]\n",
    "    \n",
    "    ax.bxp(box_data, showmeans=True)\n",
    "    ax.set_xticklabels([\"Normal Samples\", \"Fraud Samples\"], fontsize=10)\n",
    "    \n",
    "    model_name, eval_type = key\n",
    "    ax.set_title(f\"{model_name} - {eval_type}\", fontsize=14)\n",
    "    ax.set_ylabel(\"Reconstruction Loss\", fontsize=12)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"recon_error_analysis/detailed_grouped_boxplots_log.png\", dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit_vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
