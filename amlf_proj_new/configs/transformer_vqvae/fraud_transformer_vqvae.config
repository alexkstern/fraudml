[Transformer_VQVAE]
input_dim = 30
hidden_dim = 16          # Not used in this model, but provided for consistency
recon_weight = 1.0
vq_commitment_cost = 0.25
num_embeddings = 64      # Fewer embeddings drastically reduce the codebook size
embedding_dim = 20       # Must match d_model
d_model = 20             # Smaller transformer token dimension
nhead = 2                # 20 is divisible by 2
encoder_layers = 2       # Fewer encoder layers
encoder_ff_dim = 50      # Reduced feed-forward dimension
decoder_layers = 1       # Just one decoder layer
decoder_ff_dim = 50      # Reduced feed-forward dimension for decoder


[DataLoader]
dataset_name = stanpony/european_credit_card_fraud_dataset
exclude_cols = original_index, Class
normalize_cols = Time, Amount
batch_size = 32
shuffle = True
num_workers = 4
normalize = True
class_filter = 1
conv = True

[Trainer]
num_epochs = 300
patience = 60
lr = 1e-3
use_scheduler = True
scheduler_type = cosine

[WandB]
use_wandb = True
project = fraud-classification
name = transformer-vqvae-fraud
entity = alexkstern
tags = fraud, transformer, vqvae